{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4： 情感分析**\n",
    "\n",
    "随着词向量的训练，我们准备展示一个简单的情感分析案例。随着词向量的训练，我们准备展示一个简单的情感分析。对于每条Stanford Sentiment Treebank数据集中的句子，将句子中全体词向量的平均值算作其特征值，并试图预测所提句子中的情感层次。短语的情感层次使用真实数值在原始数据集中表示，并被我们用以下5个类别来表示： \n",
    "“超级消极”，“比较消极”，“中立”，“积极”，“非常积极”\n",
    "\n",
    "对其分别进行从0到4的编码。在这一部分，你将学习用SGD来训练一个softmax回归机，并且通过不断地训练／调试验证来提高回归机的泛化能力。 \n",
    "\n",
    "\n",
    "在斯坦福情感树库做5个类别的情感分析，模型是简单的softmax，仅要求准确率至少36.5%。\n",
    "\n",
    "参考资料：\n",
    "\n",
    "http://www.hankcs.com/nlp/cs224n-assignment-1.html/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1 特征向量\n",
    "\n",
    "\n",
    "实现一个句子的特征生成器和softmax回归机。\n",
    "\n",
    "一种最简单的特征选择方法就是取所有词向量的平均："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from utils.treebank import StanfordSentiment\n",
    "import utils.glove as glove\n",
    "\n",
    "from q3_sgd import load_saved_params, sgd\n",
    "\n",
    "# We will use sklearn here because it will run faster than implementing\n",
    "# ourselves. However, for other parts of this assignment you must implement\n",
    "# the functions yourself!\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def getArguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    group = parser.add_mutually_exclusive_group(required=True)\n",
    "    group.add_argument(\"--pretrained\", dest=\"pretrained\", action=\"store_true\",\n",
    "                       help=\"Use pretrained GloVe vectors.\")\n",
    "    group.add_argument(\"--yourvectors\", dest=\"yourvectors\", action=\"store_true\",\n",
    "                       help=\"Use your vectors from q3.\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "#获取句子的特征\n",
    "def getSentenceFeatures(tokens, wordVectors, sentence):\n",
    "    \"\"\"\n",
    "    Obtain the sentence feature for sentiment analysis by averaging its\n",
    "    word vectors\n",
    "    \"\"\"\n",
    "\n",
    "    # Implement computation for the sentence features given a sentence.\n",
    "\n",
    "    # Inputs:\n",
    "    # tokens -- a dictionary that maps words to their indices in\n",
    "    #           the word vector list\n",
    "    # wordVectors -- word vectors (each row) for all tokens\n",
    "    # sentence -- a list of words in the sentence of interest\n",
    "\n",
    "    # Output:\n",
    "    # - sentVector: feature vector for the sentence\n",
    "\n",
    "    sentVector = np.zeros((wordVectors.shape[1],))\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    for s in sentence:\n",
    "        sentVector += wordVectors[tokens[s], :]\n",
    "\n",
    "    sentVector *= 1.0 / len(sentence)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    assert sentVector.shape == (wordVectors.shape[1],)\n",
    "    return sentVector\n",
    "\n",
    "\n",
    "def getRegularizationValues():\n",
    "    \"\"\"Try different regularizations\n",
    "\n",
    "    Return a sorted list of values to try.\n",
    "    \"\"\"\n",
    "    values = None  # Assign a list of floats in the block below\n",
    "    ### YOUR CODE HERE\n",
    "    values = np.logspace(-4, 2, num=100, base=10)\n",
    "    ### END YOUR CODE\n",
    "    return sorted(values)\n",
    "\n",
    "\n",
    "def chooseBestModel(results):\n",
    "    \"\"\"Choose the best model based on parameter tuning on the dev set\n",
    "\n",
    "    Arguments:\n",
    "    results -- A list of python dictionaries of the following format:\n",
    "        {\n",
    "            \"reg\": regularization,\n",
    "            \"clf\": classifier,\n",
    "            \"train\": trainAccuracy,\n",
    "            \"dev\": devAccuracy,\n",
    "            \"test\": testAccuracy\n",
    "        }\n",
    "\n",
    "    Returns:\n",
    "    Your chosen result dictionary.\n",
    "    \"\"\"\n",
    "    bestResult = None\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    bestResult = max(results, key=lambda x: x[\"dev\"])\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return bestResult\n",
    "\n",
    "\n",
    "def accuracy(y, yhat):\n",
    "    \"\"\" Precision for classifier \"\"\"\n",
    "    assert (y.shape == yhat.shape)\n",
    "    return np.sum(y == yhat) * 100.0 / y.size\n",
    "\n",
    "\n",
    "def plotRegVsAccuracy(regValues, results, filename):\n",
    "    \"\"\" Make a plot of regularization vs accuracy \"\"\"\n",
    "    plt.plot(regValues, [x[\"train\"] for x in results])\n",
    "    plt.plot(regValues, [x[\"dev\"] for x in results])\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(\"regularization\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend(['train', 'dev'], loc='upper left')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "def outputConfusionMatrix(features, labels, clf, filename):\n",
    "    \"\"\" Generate a confusion matrix \"\"\"\n",
    "    pred = clf.predict(features)\n",
    "    cm = confusion_matrix(labels, pred, labels=range(5))\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Reds)\n",
    "    plt.colorbar()\n",
    "    classes = [\"- -\", \"-\", \"neut\", \"+\", \"+ +\"]\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "def outputPredictions(dataset, features, labels, clf, filename):\n",
    "    \"\"\" Write the predictions to file \"\"\"\n",
    "    pred = clf.predict(features)\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"True\\tPredicted\\tText\")\n",
    "        # print(>> f, )\n",
    "        for i in range(len(dataset)):\n",
    "        #print(s, end=\"\", file=depend)\n",
    "            f.write(\"%d\\t%d\\t%s\" % (\n",
    "                labels[i], pred[i], \" \".join(dataset[i][0])))\n",
    "\n",
    "\n",
    "def main(par):\n",
    "    \"\"\" Train a model to do sentiment analyis\"\"\"\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = StanfordSentiment()\n",
    "    tokens = dataset.tokens()\n",
    "    nWords = len(tokens)\n",
    "\n",
    "    if par==\"yourvectors\":\n",
    "        _, wordVectors, _ = load_saved_params()\n",
    "        wordVectors = np.concatenate(\n",
    "            (wordVectors[:nWords, :], wordVectors[nWords:, :]),\n",
    "            axis=1)\n",
    "    elif par==\"pretrained\":\n",
    "        wordVectors = glove.loadWordVectors(tokens)\n",
    "    dimVectors = wordVectors.shape[1]\n",
    "\n",
    "    # Load the train set\n",
    "    trainset = dataset.getTrainSentences()\n",
    "    nTrain = len(trainset)\n",
    "    trainFeatures = np.zeros((nTrain, dimVectors))\n",
    "    trainLabels = np.zeros((nTrain,), dtype=np.int32)\n",
    "    for i in range(nTrain):\n",
    "        words, trainLabels[i] = trainset[i]\n",
    "        trainFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n",
    "\n",
    "    # Prepare dev set features\n",
    "    devset = dataset.getDevSentences()\n",
    "    nDev = len(devset)\n",
    "    devFeatures = np.zeros((nDev, dimVectors))\n",
    "    devLabels = np.zeros((nDev,), dtype=np.int32)\n",
    "    for i in range(nDev):\n",
    "        words, devLabels[i] = devset[i]\n",
    "        devFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n",
    "\n",
    "    # Prepare test set features\n",
    "    testset = dataset.getTestSentences()\n",
    "    nTest = len(testset)\n",
    "    testFeatures = np.zeros((nTest, dimVectors))\n",
    "    testLabels = np.zeros((nTest,), dtype=np.int32)\n",
    "    for i in range(nTest):\n",
    "        words, testLabels[i] = testset[i]\n",
    "        testFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n",
    "\n",
    "    # We will save our results from each run\n",
    "    results = []\n",
    "    regValues = getRegularizationValues()\n",
    "    for reg in regValues:\n",
    "        print(\"Training for reg=%f\" % reg)\n",
    "        # Note: add a very small number to regularization to please the library\n",
    "        clf = LogisticRegression(C=1.0 / (reg + 1e-12))\n",
    "        clf.fit(trainFeatures, trainLabels)\n",
    "\n",
    "        # Test on train set\n",
    "        pred = clf.predict(trainFeatures)\n",
    "        trainAccuracy = accuracy(trainLabels, pred)\n",
    "        print(\"Train accuracy (%%): %f\" % trainAccuracy)\n",
    "\n",
    "        # Test on dev set\n",
    "        pred = clf.predict(devFeatures)\n",
    "        devAccuracy = accuracy(devLabels, pred)\n",
    "        print(\"Dev accuracy (%%): %f\" % devAccuracy)\n",
    "\n",
    "        # Test on test set\n",
    "        # Note: always running on test is poor style. Typically, you should\n",
    "        # do this only after validation.\n",
    "        pred = clf.predict(testFeatures)\n",
    "        testAccuracy = accuracy(testLabels, pred)\n",
    "        print(\"Test accuracy (%%): %f\" % testAccuracy)\n",
    "\n",
    "        results.append({\n",
    "            \"reg\": reg,\n",
    "            \"clf\": clf,\n",
    "            \"train\": trainAccuracy,\n",
    "            \"dev\": devAccuracy,\n",
    "            \"test\": testAccuracy})\n",
    "\n",
    "    # Print the accuracies\n",
    "    print(\"\")\n",
    "    print(\"=== Recap ===\")\n",
    "    print(\"Reg\\t\\tTrain\\tDev\\tTest\")\n",
    "    for result in results:\n",
    "        print(\"%.2E\\t%.3f\\t%.3f\\t%.3f\" % (\n",
    "            result[\"reg\"],\n",
    "            result[\"train\"],\n",
    "            result[\"dev\"],\n",
    "            result[\"test\"]))\n",
    "    print(\"\")\n",
    "\n",
    "    bestResult = chooseBestModel(results)\n",
    "    print(\"Best regularization value: %0.2E\" % bestResult[\"reg\"])\n",
    "    print(\"Test accuracy (%%): %f\" % bestResult[\"test\"])\n",
    "\n",
    "    # do some error analysis\n",
    "    if args==\"pretrained\":\n",
    "        plotRegVsAccuracy(regValues, results, \"q4_reg_v_acc.png\")\n",
    "        outputConfusionMatrix(devFeatures, devLabels, bestResult[\"clf\"],\"q4_dev_conf.png\")\n",
    "        outputPredictions(devset, devFeatures, devLabels, bestResult[\"clf\"], \"q4_dev_pred.txt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ipykernel_launcher.py main(getArguments()) -h --pretrained  \n",
    "args=\"yourvectors\"\n",
    "main(args)\n",
    "\n",
    "#如果用训练不收敛的数据结果：\n",
    "#Best regularization value: 1.15E-04\n",
    "#Test accuracy (%): 29.502262\n",
    "\n",
    "#自己训练的词向量\n",
    "#Best regularization value: 1.15E-04\n",
    "#Test accuracy (%): 29.502262\n",
    "\n",
    "#http://liuchengxu.org/pelican-blog/jupyter-notebook-tips.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4-2 正则化的意义\n",
    "\n",
    "解释当分类语料少于三句时为什么要引入正则化（实际上在大多数机器学习任务都这样）。\n",
    "\n",
    "避免过拟合，提高对未知实例的泛化能力\n",
    "\n",
    "## 4-3 调参\n",
    "\n",
    "在`q4 sentiment.py`中完成超参数的实现代码从而获取“最佳”的惩罚因子。你是如何选择的？报告你的训练、调试和测试精度，在最多一个句子中校正你的超参数选定方法。 注释：在开发中应该获取至少30%的准确率。 \n",
    "\n",
    "解答：参考值为1e-4，在调试、开发和测试过程中准确率分别为27.072%， 25.341%，22.896%\n",
    "\n",
    "在验证集上搜索超参数代码如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRegularizationValues():\n",
    "    \"\"\"Try different regularizations\n",
    "    Return a sorted list of values to try.\n",
    "    \"\"\"\n",
    "    values = None  # Assign a list of floats in the block below\n",
    "    ### YOUR CODE HERE\n",
    "    values = np.logspace(-4, 2, num=100, base=10)\n",
    "    ### END YOUR CODE\n",
    "    return sorted(values)\n",
    "\n",
    "def chooseBestModel(results):\n",
    "    \"\"\"Choose the best model based on parameter tuning on the dev set\n",
    "    Arguments:\n",
    "    results -- A list of python dictionaries of the following format:\n",
    "        {\n",
    "            \"reg\": regularization,\n",
    "            \"clf\": classifier,\n",
    "            \"train\": trainAccuracy,\n",
    "            \"dev\": devAccuracy,\n",
    "            \"test\": testAccuracy\n",
    "        }\n",
    "    Returns:\n",
    "    Your chosen result dictionary.\n",
    "    \"\"\"\n",
    "    bestResult = None\n",
    "    ### YOUR CODE HERE\n",
    "    bestResult = max(results, key=lambda x: x[\"dev\"])\n",
    "    ### END YOUR CODE\n",
    "    return bestResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-4 词向量的影响\n",
    "\n",
    "绘出在训练和开发过程中的分类准确率，并在x轴使用对数刻度来对正则化值进行相关设置。这应该自动化的进行。包括在你作业中详细展示的坐标图`q4_reg_acc.png`。简明解释最多三个句子在此坐标图中的显示情况。 \n",
    "\n",
    "\n",
    "\n",
    "用自己的词向量训练模型，与用GloVe预训练的词向量的模型作比较，为什么后者好？\n",
    "\n",
    "后者更好的原因是：\n",
    "\n",
    "- 后者在维基上训练，数据量更大\n",
    "\n",
    "- 后者维度更高（50维）\n",
    "\n",
    "- GloVe利用了全局统计信息，而word2vec（SG）没有\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-5 惩罚因子对效果的影响\n",
    "\n",
    "从程序运行完保存的`q4_reg_v_acc.png`可以看出正则化的惩罚因子对结果的影响：\n",
    "![q4_reg_v_acc.png](q4_reg_v_acc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用预训练的Glove模型\n",
    "args=\"pretrained\"\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然用预训练的词向量，效果更高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un_expect_num= 107\n",
      "Training for reg=0.000000\n",
      "iter 100: 1.575228\n",
      "iter 200: 1.575209\n",
      "iter 300: 1.575174\n",
      "iter 400: 1.575125\n",
      "iter 500: 1.575062\n",
      "iter 600: 1.574987\n",
      "iter 700: 1.574902\n",
      "iter 800: 1.574808\n",
      "iter 900: 1.574705\n",
      "iter 1000: 1.574595\n",
      "iter 1100: 1.574479\n",
      "iter 1200: 1.574358\n",
      "iter 1300: 1.574231\n",
      "iter 1400: 1.574100\n",
      "iter 1500: 1.573966\n",
      "iter 1600: 1.573829\n",
      "iter 1700: 1.573689\n",
      "iter 1800: 1.573548\n",
      "iter 1900: 1.573404\n",
      "iter 2000: 1.573259\n",
      "iter 2100: 1.573113\n",
      "iter 2200: 1.572967\n",
      "iter 2300: 1.572820\n",
      "iter 2400: 1.572673\n",
      "iter 2500: 1.572526\n",
      "iter 2600: 1.572379\n",
      "iter 2700: 1.572233\n",
      "iter 2800: 1.572087\n",
      "iter 2900: 1.571942\n",
      "iter 3000: 1.571798\n",
      "iter 3100: 1.571655\n",
      "iter 3200: 1.571512\n",
      "iter 3300: 1.571371\n",
      "iter 3400: 1.571231\n",
      "iter 3500: 1.571092\n",
      "iter 3600: 1.570955\n",
      "iter 3700: 1.570819\n",
      "iter 3800: 1.570685\n",
      "iter 3900: 1.570551\n",
      "iter 4000: 1.570420\n",
      "iter 4100: 1.570290\n",
      "iter 4200: 1.570161\n",
      "iter 4300: 1.570034\n",
      "iter 4400: 1.569908\n",
      "iter 4500: 1.569784\n",
      "iter 4600: 1.569662\n",
      "iter 4700: 1.569541\n",
      "iter 4800: 1.569422\n",
      "iter 4900: 1.569304\n",
      "iter 5000: 1.569188\n",
      "iter 5100: 1.569073\n",
      "iter 5200: 1.568960\n",
      "iter 5300: 1.568849\n",
      "iter 5400: 1.568739\n",
      "iter 5500: 1.568631\n",
      "iter 5600: 1.568524\n",
      "iter 5700: 1.568418\n",
      "iter 5800: 1.568314\n",
      "iter 5900: 1.568212\n",
      "iter 6000: 1.568111\n",
      "iter 6100: 1.568011\n",
      "iter 6200: 1.567913\n",
      "iter 6300: 1.567816\n",
      "iter 6400: 1.567720\n",
      "iter 6500: 1.567626\n",
      "iter 6600: 1.567533\n",
      "iter 6700: 1.567442\n",
      "iter 6800: 1.567352\n",
      "iter 6900: 1.567263\n",
      "iter 7000: 1.567175\n",
      "iter 7100: 1.567089\n",
      "iter 7200: 1.567004\n",
      "iter 7300: 1.566920\n",
      "iter 7400: 1.566837\n",
      "iter 7500: 1.566756\n",
      "iter 7600: 1.566675\n",
      "iter 7700: 1.566596\n",
      "iter 7800: 1.566518\n",
      "iter 7900: 1.566441\n",
      "iter 8000: 1.566365\n",
      "iter 8100: 1.566290\n",
      "iter 8200: 1.566216\n",
      "iter 8300: 1.566144\n",
      "iter 8400: 1.566072\n",
      "iter 8500: 1.566001\n",
      "iter 8600: 1.565932\n",
      "iter 8700: 1.565863\n",
      "iter 8800: 1.565795\n",
      "iter 8900: 1.565728\n",
      "iter 9000: 1.565662\n",
      "iter 9100: 1.565597\n",
      "iter 9200: 1.565533\n",
      "iter 9300: 1.565470\n",
      "iter 9400: 1.565408\n",
      "iter 9500: 1.565346\n",
      "iter 9600: 1.565286\n",
      "iter 9700: 1.565226\n",
      "iter 9800: 1.565167\n",
      "iter 9900: 1.565109\n",
      "iter 10000: 1.565052\n",
      "Train accuracy (%): 28.862360\n",
      "Dev accuracy (%): 30.245232\n",
      "Training for reg=0.000010\n",
      "iter 100: 1.575448\n",
      "iter 200: 1.575429\n",
      "iter 300: 1.575393\n",
      "iter 400: 1.575343\n",
      "iter 500: 1.575279\n",
      "iter 600: 1.575203\n",
      "iter 700: 1.575117\n",
      "iter 800: 1.575022\n",
      "iter 900: 1.574919\n",
      "iter 1000: 1.574810\n",
      "iter 1100: 1.574694\n",
      "iter 1200: 1.574573\n",
      "iter 1300: 1.574447\n",
      "iter 1400: 1.574318\n",
      "iter 1500: 1.574186\n",
      "iter 1600: 1.574051\n",
      "iter 1700: 1.573914\n",
      "iter 1800: 1.573775\n",
      "iter 1900: 1.573635\n",
      "iter 2000: 1.573495\n",
      "iter 2100: 1.573354\n",
      "iter 2200: 1.573212\n",
      "iter 2300: 1.573071\n",
      "iter 2400: 1.572930\n",
      "iter 2500: 1.572790\n",
      "iter 2600: 1.572650\n",
      "iter 2700: 1.572511\n",
      "iter 2800: 1.572373\n",
      "iter 2900: 1.572236\n",
      "iter 3000: 1.572100\n",
      "iter 3100: 1.571966\n",
      "iter 3200: 1.571834\n",
      "iter 3300: 1.571702\n",
      "iter 3400: 1.571573\n",
      "iter 3500: 1.571445\n",
      "iter 3600: 1.571318\n",
      "iter 3700: 1.571194\n",
      "iter 3800: 1.571071\n",
      "iter 3900: 1.570950\n",
      "iter 4000: 1.570831\n",
      "iter 4100: 1.570713\n",
      "iter 4200: 1.570598\n",
      "iter 4300: 1.570484\n",
      "iter 4400: 1.570372\n",
      "iter 4500: 1.570262\n",
      "iter 4600: 1.570154\n",
      "iter 4700: 1.570047\n",
      "iter 4800: 1.569943\n",
      "iter 4900: 1.569840\n",
      "iter 5000: 1.569739\n",
      "iter 5100: 1.569640\n",
      "iter 5200: 1.569543\n",
      "iter 5300: 1.569447\n",
      "iter 5400: 1.569353\n",
      "iter 5500: 1.569261\n",
      "iter 5600: 1.569171\n",
      "iter 5700: 1.569082\n",
      "iter 5800: 1.568995\n",
      "iter 5900: 1.568909\n",
      "iter 6000: 1.568825\n",
      "iter 6100: 1.568743\n",
      "iter 6200: 1.568662\n",
      "iter 6300: 1.568583\n",
      "iter 6400: 1.568505\n",
      "iter 6500: 1.568429\n",
      "iter 6600: 1.568354\n",
      "iter 6700: 1.568280\n",
      "iter 6800: 1.568208\n",
      "iter 6900: 1.568138\n",
      "iter 7000: 1.568068\n",
      "iter 7100: 1.568001\n",
      "iter 7200: 1.567934\n",
      "iter 7300: 1.567869\n",
      "iter 7400: 1.567805\n",
      "iter 7500: 1.567742\n",
      "iter 7600: 1.567680\n",
      "iter 7700: 1.567620\n",
      "iter 7800: 1.567560\n",
      "iter 7900: 1.567502\n",
      "iter 8000: 1.567445\n",
      "iter 8100: 1.567389\n",
      "iter 8200: 1.567335\n",
      "iter 8300: 1.567281\n",
      "iter 8400: 1.567228\n",
      "iter 8500: 1.567177\n",
      "iter 8600: 1.567126\n",
      "iter 8700: 1.567076\n",
      "iter 8800: 1.567028\n",
      "iter 8900: 1.566980\n",
      "iter 9000: 1.566933\n",
      "iter 9100: 1.566887\n",
      "iter 9200: 1.566842\n",
      "iter 9300: 1.566798\n",
      "iter 9400: 1.566755\n",
      "iter 9500: 1.566712\n",
      "iter 9600: 1.566671\n",
      "iter 9700: 1.566630\n",
      "iter 9800: 1.566590\n",
      "iter 9900: 1.566550\n",
      "iter 10000: 1.566512\n",
      "Train accuracy (%): 28.768727\n",
      "Dev accuracy (%): 29.972752\n",
      "Training for reg=0.000030\n",
      "iter 100: 1.575880\n",
      "iter 200: 1.575860\n",
      "iter 300: 1.575822\n",
      "iter 400: 1.575768\n",
      "iter 500: 1.575701\n",
      "iter 600: 1.575623\n",
      "iter 700: 1.575533\n",
      "iter 800: 1.575435\n",
      "iter 900: 1.575330\n",
      "iter 1000: 1.575217\n",
      "iter 1100: 1.575100\n",
      "iter 1200: 1.574977\n",
      "iter 1300: 1.574851\n",
      "iter 1400: 1.574722\n",
      "iter 1500: 1.574590\n",
      "iter 1600: 1.574456\n",
      "iter 1700: 1.574322\n",
      "iter 1800: 1.574186\n",
      "iter 1900: 1.574050\n",
      "iter 2000: 1.573914\n",
      "iter 2100: 1.573778\n",
      "iter 2200: 1.573643\n",
      "iter 2300: 1.573508\n",
      "iter 2400: 1.573375\n",
      "iter 2500: 1.573242\n",
      "iter 2600: 1.573112\n",
      "iter 2700: 1.572982\n",
      "iter 2800: 1.572855\n",
      "iter 2900: 1.572729\n",
      "iter 3000: 1.572605\n",
      "iter 3100: 1.572483\n",
      "iter 3200: 1.572364\n",
      "iter 3300: 1.572246\n",
      "iter 3400: 1.572130\n",
      "iter 3500: 1.572017\n",
      "iter 3600: 1.571906\n",
      "iter 3700: 1.571797\n",
      "iter 3800: 1.571690\n",
      "iter 3900: 1.571586\n",
      "iter 4000: 1.571484\n",
      "iter 4100: 1.571384\n",
      "iter 4200: 1.571286\n",
      "iter 4300: 1.571191\n",
      "iter 4400: 1.571098\n",
      "iter 4500: 1.571007\n",
      "iter 4600: 1.570918\n",
      "iter 4700: 1.570831\n",
      "iter 4800: 1.570747\n",
      "iter 4900: 1.570664\n",
      "iter 5000: 1.570584\n",
      "iter 5100: 1.570505\n",
      "iter 5200: 1.570429\n",
      "iter 5300: 1.570354\n",
      "iter 5400: 1.570282\n",
      "iter 5500: 1.570211\n",
      "iter 5600: 1.570142\n",
      "iter 5700: 1.570075\n",
      "iter 5800: 1.570009\n",
      "iter 5900: 1.569946\n",
      "iter 6000: 1.569884\n",
      "iter 6100: 1.569824\n",
      "iter 6200: 1.569765\n",
      "iter 6300: 1.569708\n",
      "iter 6400: 1.569653\n",
      "iter 6500: 1.569599\n",
      "iter 6600: 1.569546\n",
      "iter 6700: 1.569495\n",
      "iter 6800: 1.569445\n",
      "iter 6900: 1.569397\n",
      "iter 7000: 1.569350\n",
      "iter 7100: 1.569304\n",
      "iter 7200: 1.569260\n",
      "iter 7300: 1.569217\n",
      "iter 7400: 1.569175\n",
      "iter 7500: 1.569134\n",
      "iter 7600: 1.569095\n",
      "iter 7700: 1.569056\n",
      "iter 7800: 1.569019\n",
      "iter 7900: 1.568983\n",
      "iter 8000: 1.568947\n",
      "iter 8100: 1.568913\n",
      "iter 8200: 1.568880\n",
      "iter 8300: 1.568848\n",
      "iter 8400: 1.568816\n",
      "iter 8500: 1.568786\n",
      "iter 8600: 1.568756\n",
      "iter 8700: 1.568727\n",
      "iter 8800: 1.568699\n",
      "iter 8900: 1.568672\n",
      "iter 9000: 1.568646\n",
      "iter 9100: 1.568620\n",
      "iter 9200: 1.568595\n",
      "iter 9300: 1.568571\n",
      "iter 9400: 1.568548\n",
      "iter 9500: 1.568525\n",
      "iter 9600: 1.568503\n",
      "iter 9700: 1.568482\n",
      "iter 9800: 1.568461\n",
      "iter 9900: 1.568441\n",
      "iter 10000: 1.568421\n",
      "Train accuracy (%): 28.651685\n",
      "Dev accuracy (%): 29.881926\n",
      "Training for reg=0.000100\n",
      "iter 100: 1.577312\n",
      "iter 200: 1.577284\n",
      "iter 300: 1.577232\n",
      "iter 400: 1.577161\n",
      "iter 500: 1.577072\n",
      "iter 600: 1.576969\n",
      "iter 700: 1.576855\n",
      "iter 800: 1.576731\n",
      "iter 900: 1.576600\n",
      "iter 1000: 1.576462\n",
      "iter 1100: 1.576321\n",
      "iter 1200: 1.576176\n",
      "iter 1300: 1.576028\n",
      "iter 1400: 1.575880\n",
      "iter 1500: 1.575731\n",
      "iter 1600: 1.575582\n",
      "iter 1700: 1.575434\n",
      "iter 1800: 1.575287\n",
      "iter 1900: 1.575142\n",
      "iter 2000: 1.574999\n",
      "iter 2100: 1.574859\n",
      "iter 2200: 1.574721\n",
      "iter 2300: 1.574587\n",
      "iter 2400: 1.574455\n",
      "iter 2500: 1.574327\n",
      "iter 2600: 1.574202\n",
      "iter 2700: 1.574080\n",
      "iter 2800: 1.573962\n",
      "iter 2900: 1.573847\n",
      "iter 3000: 1.573736\n",
      "iter 3100: 1.573628\n",
      "iter 3200: 1.573524\n",
      "iter 3300: 1.573423\n",
      "iter 3400: 1.573325\n",
      "iter 3500: 1.573231\n",
      "iter 3600: 1.573140\n",
      "iter 3700: 1.573053\n",
      "iter 3800: 1.572968\n",
      "iter 3900: 1.572887\n",
      "iter 4000: 1.572809\n",
      "iter 4100: 1.572733\n",
      "iter 4200: 1.572661\n",
      "iter 4300: 1.572591\n",
      "iter 4400: 1.572524\n",
      "iter 4500: 1.572460\n",
      "iter 4600: 1.572398\n",
      "iter 4700: 1.572339\n",
      "iter 4800: 1.572282\n",
      "iter 4900: 1.572227\n",
      "iter 5000: 1.572175\n",
      "iter 5100: 1.572124\n",
      "iter 5200: 1.572076\n",
      "iter 5300: 1.572030\n",
      "iter 5400: 1.571986\n",
      "iter 5500: 1.571944\n",
      "iter 5600: 1.571903\n",
      "iter 5700: 1.571864\n",
      "iter 5800: 1.571827\n",
      "iter 5900: 1.571792\n",
      "iter 6000: 1.571758\n",
      "iter 6100: 1.571725\n",
      "iter 6200: 1.571694\n",
      "iter 6300: 1.571664\n",
      "iter 6400: 1.571636\n",
      "iter 6500: 1.571609\n",
      "iter 6600: 1.571583\n",
      "iter 6700: 1.571558\n",
      "iter 6800: 1.571534\n",
      "iter 6900: 1.571512\n",
      "iter 7000: 1.571490\n",
      "iter 7100: 1.571469\n",
      "iter 7200: 1.571450\n",
      "iter 7300: 1.571431\n",
      "iter 7400: 1.571413\n",
      "iter 7500: 1.571396\n",
      "iter 7600: 1.571379\n",
      "iter 7700: 1.571364\n",
      "iter 7800: 1.571349\n",
      "iter 7900: 1.571334\n",
      "iter 8000: 1.571321\n",
      "iter 8100: 1.571308\n",
      "iter 8200: 1.571295\n",
      "iter 8300: 1.571284\n",
      "iter 8400: 1.571272\n",
      "iter 8500: 1.571262\n",
      "iter 8600: 1.571252\n",
      "iter 8700: 1.571242\n",
      "iter 8800: 1.571233\n",
      "iter 8900: 1.571224\n",
      "iter 9000: 1.571215\n",
      "iter 9100: 1.571207\n",
      "iter 9200: 1.571200\n",
      "iter 9300: 1.571192\n",
      "iter 9400: 1.571185\n",
      "iter 9500: 1.571179\n",
      "iter 9600: 1.571172\n",
      "iter 9700: 1.571166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9800: 1.571161\n",
      "iter 9900: 1.571155\n",
      "iter 10000: 1.571150\n",
      "Train accuracy (%): 28.604869\n",
      "Dev accuracy (%): 29.155313\n",
      "Training for reg=0.000300\n",
      "iter 100: 1.580777\n",
      "iter 200: 1.580704\n",
      "iter 300: 1.580575\n",
      "iter 400: 1.580405\n",
      "iter 500: 1.580204\n",
      "iter 600: 1.579982\n",
      "iter 700: 1.579745\n",
      "iter 800: 1.579498\n",
      "iter 900: 1.579246\n",
      "iter 1000: 1.578993\n",
      "iter 1100: 1.578741\n",
      "iter 1200: 1.578493\n",
      "iter 1300: 1.578249\n",
      "iter 1400: 1.578010\n",
      "iter 1500: 1.577779\n",
      "iter 1600: 1.577555\n",
      "iter 1700: 1.577339\n",
      "iter 1800: 1.577130\n",
      "iter 1900: 1.576930\n",
      "iter 2000: 1.576738\n",
      "iter 2100: 1.576553\n",
      "iter 2200: 1.576377\n",
      "iter 2300: 1.576209\n",
      "iter 2400: 1.576047\n",
      "iter 2500: 1.575894\n",
      "iter 2600: 1.575747\n",
      "iter 2700: 1.575607\n",
      "iter 2800: 1.575474\n",
      "iter 2900: 1.575347\n",
      "iter 3000: 1.575227\n",
      "iter 3100: 1.575112\n",
      "iter 3200: 1.575002\n",
      "iter 3300: 1.574898\n",
      "iter 3400: 1.574799\n",
      "iter 3500: 1.574705\n",
      "iter 3600: 1.574615\n",
      "iter 3700: 1.574530\n",
      "iter 3800: 1.574449\n",
      "iter 3900: 1.574372\n",
      "iter 4000: 1.574299\n",
      "iter 4100: 1.574230\n",
      "iter 4200: 1.574163\n",
      "iter 4300: 1.574101\n",
      "iter 4400: 1.574041\n",
      "iter 4500: 1.573984\n",
      "iter 4600: 1.573930\n",
      "iter 4700: 1.573879\n",
      "iter 4800: 1.573831\n",
      "iter 4900: 1.573784\n",
      "iter 5000: 1.573740\n",
      "iter 5100: 1.573699\n",
      "iter 5200: 1.573659\n",
      "iter 5300: 1.573621\n",
      "iter 5400: 1.573586\n",
      "iter 5500: 1.573552\n",
      "iter 5600: 1.573519\n",
      "iter 5700: 1.573489\n",
      "iter 5800: 1.573459\n",
      "iter 5900: 1.573432\n",
      "iter 6000: 1.573405\n",
      "iter 6100: 1.573380\n",
      "iter 6200: 1.573357\n",
      "iter 6300: 1.573334\n",
      "iter 6400: 1.573313\n",
      "iter 6500: 1.573292\n",
      "iter 6600: 1.573273\n",
      "iter 6700: 1.573255\n",
      "iter 6800: 1.573237\n",
      "iter 6900: 1.573221\n",
      "iter 7000: 1.573205\n",
      "iter 7100: 1.573190\n",
      "iter 7200: 1.573176\n",
      "iter 7300: 1.573162\n",
      "iter 7400: 1.573149\n",
      "iter 7500: 1.573137\n",
      "iter 7600: 1.573125\n",
      "iter 7700: 1.573114\n",
      "iter 7800: 1.573104\n",
      "iter 7900: 1.573094\n",
      "iter 8000: 1.573085\n",
      "iter 8100: 1.573076\n",
      "iter 8200: 1.573067\n",
      "iter 8300: 1.573059\n",
      "iter 8400: 1.573051\n",
      "iter 8500: 1.573044\n",
      "iter 8600: 1.573037\n",
      "iter 8700: 1.573031\n",
      "iter 8800: 1.573024\n",
      "iter 8900: 1.573018\n",
      "iter 9000: 1.573013\n",
      "iter 9100: 1.573007\n",
      "iter 9200: 1.573002\n",
      "iter 9300: 1.572997\n",
      "iter 9400: 1.572993\n",
      "iter 9500: 1.572988\n",
      "iter 9600: 1.572984\n",
      "iter 9700: 1.572980\n",
      "iter 9800: 1.572977\n",
      "iter 9900: 1.572973\n",
      "iter 10000: 1.572970\n",
      "Train accuracy (%): 27.949438\n",
      "Dev accuracy (%): 26.793824\n",
      "Training for reg=0.001000\n",
      "iter 100: 1.587395\n",
      "iter 200: 1.587085\n",
      "iter 300: 1.586624\n",
      "iter 400: 1.586097\n",
      "iter 500: 1.585547\n",
      "iter 600: 1.584999\n",
      "iter 700: 1.584465\n",
      "iter 800: 1.583949\n",
      "iter 900: 1.583455\n",
      "iter 1000: 1.582984\n",
      "iter 1100: 1.582535\n",
      "iter 1200: 1.582108\n",
      "iter 1300: 1.581701\n",
      "iter 1400: 1.581315\n",
      "iter 1500: 1.580949\n",
      "iter 1600: 1.580600\n",
      "iter 1700: 1.580269\n",
      "iter 1800: 1.579954\n",
      "iter 1900: 1.579655\n",
      "iter 2000: 1.579371\n",
      "iter 2100: 1.579102\n",
      "iter 2200: 1.578845\n",
      "iter 2300: 1.578602\n",
      "iter 2400: 1.578371\n",
      "iter 2500: 1.578151\n",
      "iter 2600: 1.577942\n",
      "iter 2700: 1.577744\n",
      "iter 2800: 1.577555\n",
      "iter 2900: 1.577376\n",
      "iter 3000: 1.577206\n",
      "iter 3100: 1.577045\n",
      "iter 3200: 1.576891\n",
      "iter 3300: 1.576746\n",
      "iter 3400: 1.576607\n",
      "iter 3500: 1.576476\n",
      "iter 3600: 1.576351\n",
      "iter 3700: 1.576232\n",
      "iter 3800: 1.576119\n",
      "iter 3900: 1.576012\n",
      "iter 4000: 1.575910\n",
      "iter 4100: 1.575813\n",
      "iter 4200: 1.575721\n",
      "iter 4300: 1.575634\n",
      "iter 4400: 1.575551\n",
      "iter 4500: 1.575473\n",
      "iter 4600: 1.575398\n",
      "iter 4700: 1.575327\n",
      "iter 4800: 1.575259\n",
      "iter 4900: 1.575195\n",
      "iter 5000: 1.575134\n",
      "iter 5100: 1.575076\n",
      "iter 5200: 1.575021\n",
      "iter 5300: 1.574969\n",
      "iter 5400: 1.574919\n",
      "iter 5500: 1.574872\n",
      "iter 5600: 1.574827\n",
      "iter 5700: 1.574785\n",
      "iter 5800: 1.574744\n",
      "iter 5900: 1.574706\n",
      "iter 6000: 1.574669\n",
      "iter 6100: 1.574635\n",
      "iter 6200: 1.574602\n",
      "iter 6300: 1.574570\n",
      "iter 6400: 1.574541\n",
      "iter 6500: 1.574512\n",
      "iter 6600: 1.574486\n",
      "iter 6700: 1.574460\n",
      "iter 6800: 1.574436\n",
      "iter 6900: 1.574413\n",
      "iter 7000: 1.574391\n",
      "iter 7100: 1.574370\n",
      "iter 7200: 1.574351\n",
      "iter 7300: 1.574332\n",
      "iter 7400: 1.574314\n",
      "iter 7500: 1.574297\n",
      "iter 7600: 1.574281\n",
      "iter 7700: 1.574266\n",
      "iter 7800: 1.574251\n",
      "iter 7900: 1.574238\n",
      "iter 8000: 1.574224\n",
      "iter 8100: 1.574212\n",
      "iter 8200: 1.574200\n",
      "iter 8300: 1.574189\n",
      "iter 8400: 1.574178\n",
      "iter 8500: 1.574168\n",
      "iter 8600: 1.574159\n",
      "iter 8700: 1.574149\n",
      "iter 8800: 1.574141\n",
      "iter 8900: 1.574133\n",
      "iter 9000: 1.574125\n",
      "iter 9100: 1.574117\n",
      "iter 9200: 1.574110\n",
      "iter 9300: 1.574103\n",
      "iter 9400: 1.574097\n",
      "iter 9500: 1.574091\n",
      "iter 9600: 1.574085\n",
      "iter 9700: 1.574080\n",
      "iter 9800: 1.574075\n",
      "iter 9900: 1.574070\n",
      "iter 10000: 1.574065\n",
      "Train accuracy (%): 27.118446\n",
      "Dev accuracy (%): 25.249773\n",
      "Training for reg=0.003000\n",
      "iter 100: 1.586242\n",
      "iter 200: 1.585757\n",
      "iter 300: 1.585219\n",
      "iter 400: 1.584695\n",
      "iter 500: 1.584195\n",
      "iter 600: 1.583720\n",
      "iter 700: 1.583269\n",
      "iter 800: 1.582840\n",
      "iter 900: 1.582432\n",
      "iter 1000: 1.582045\n",
      "iter 1100: 1.581678\n",
      "iter 1200: 1.581328\n",
      "iter 1300: 1.580997\n",
      "iter 1400: 1.580681\n",
      "iter 1500: 1.580382\n",
      "iter 1600: 1.580097\n",
      "iter 1700: 1.579827\n",
      "iter 1800: 1.579570\n",
      "iter 1900: 1.579326\n",
      "iter 2000: 1.579094\n",
      "iter 2100: 1.578874\n",
      "iter 2200: 1.578665\n",
      "iter 2300: 1.578466\n",
      "iter 2400: 1.578278\n",
      "iter 2500: 1.578098\n",
      "iter 2600: 1.577928\n",
      "iter 2700: 1.577766\n",
      "iter 2800: 1.577612\n",
      "iter 2900: 1.577466\n",
      "iter 3000: 1.577328\n",
      "iter 3100: 1.577196\n",
      "iter 3200: 1.577071\n",
      "iter 3300: 1.576952\n",
      "iter 3400: 1.576839\n",
      "iter 3500: 1.576731\n",
      "iter 3600: 1.576629\n",
      "iter 3700: 1.576532\n",
      "iter 3800: 1.576440\n",
      "iter 3900: 1.576353\n",
      "iter 4000: 1.576270\n",
      "iter 4100: 1.576191\n",
      "iter 4200: 1.576116\n",
      "iter 4300: 1.576045\n",
      "iter 4400: 1.575977\n",
      "iter 4500: 1.575913\n",
      "iter 4600: 1.575852\n",
      "iter 4700: 1.575794\n",
      "iter 4800: 1.575739\n",
      "iter 4900: 1.575686\n",
      "iter 5000: 1.575636\n",
      "iter 5100: 1.575589\n",
      "iter 5200: 1.575544\n",
      "iter 5300: 1.575502\n",
      "iter 5400: 1.575461\n",
      "iter 5500: 1.575423\n",
      "iter 5600: 1.575386\n",
      "iter 5700: 1.575351\n",
      "iter 5800: 1.575318\n",
      "iter 5900: 1.575287\n",
      "iter 6000: 1.575257\n",
      "iter 6100: 1.575229\n",
      "iter 6200: 1.575202\n",
      "iter 6300: 1.575176\n",
      "iter 6400: 1.575152\n",
      "iter 6500: 1.575129\n",
      "iter 6600: 1.575107\n",
      "iter 6700: 1.575086\n",
      "iter 6800: 1.575067\n",
      "iter 6900: 1.575048\n",
      "iter 7000: 1.575030\n",
      "iter 7100: 1.575013\n",
      "iter 7200: 1.574997\n",
      "iter 7300: 1.574982\n",
      "iter 7400: 1.574967\n",
      "iter 7500: 1.574953\n",
      "iter 7600: 1.574940\n",
      "iter 7700: 1.574928\n",
      "iter 7800: 1.574916\n",
      "iter 7900: 1.574905\n",
      "iter 8000: 1.574894\n",
      "iter 8100: 1.574884\n",
      "iter 8200: 1.574874\n",
      "iter 8300: 1.574865\n",
      "iter 8400: 1.574857\n",
      "iter 8500: 1.574848\n",
      "iter 8600: 1.574840\n",
      "iter 8700: 1.574833\n",
      "iter 8800: 1.574826\n",
      "iter 8900: 1.574819\n",
      "iter 9000: 1.574813\n",
      "iter 9100: 1.574807\n",
      "iter 9200: 1.574801\n",
      "iter 9300: 1.574795\n",
      "iter 9400: 1.574790\n",
      "iter 9500: 1.574785\n",
      "iter 9600: 1.574781\n",
      "iter 9700: 1.574776\n",
      "iter 9800: 1.574772\n",
      "iter 9900: 1.574768\n",
      "iter 10000: 1.574764\n",
      "Train accuracy (%): 27.083333\n",
      "Dev accuracy (%): 25.340599\n",
      "Training for reg=0.010000\n",
      "iter 100: 1.576665\n",
      "iter 200: 1.576638\n",
      "iter 300: 1.576612\n",
      "iter 400: 1.576587\n",
      "iter 500: 1.576564\n",
      "iter 600: 1.576542\n",
      "iter 700: 1.576521\n",
      "iter 800: 1.576501\n",
      "iter 900: 1.576482\n",
      "iter 1000: 1.576463\n",
      "iter 1100: 1.576446\n",
      "iter 1200: 1.576430\n",
      "iter 1300: 1.576414\n",
      "iter 1400: 1.576400\n",
      "iter 1500: 1.576386\n",
      "iter 1600: 1.576372\n",
      "iter 1700: 1.576360\n",
      "iter 1800: 1.576348\n",
      "iter 1900: 1.576336\n",
      "iter 2000: 1.576325\n",
      "iter 2100: 1.576315\n",
      "iter 2200: 1.576305\n",
      "iter 2300: 1.576296\n",
      "iter 2400: 1.576287\n",
      "iter 2500: 1.576279\n",
      "iter 2600: 1.576271\n",
      "iter 2700: 1.576263\n",
      "iter 2800: 1.576256\n",
      "iter 2900: 1.576249\n",
      "iter 3000: 1.576243\n",
      "iter 3100: 1.576237\n",
      "iter 3200: 1.576231\n",
      "iter 3300: 1.576225\n",
      "iter 3400: 1.576220\n",
      "iter 3500: 1.576215\n",
      "iter 3600: 1.576210\n",
      "iter 3700: 1.576206\n",
      "iter 3800: 1.576201\n",
      "iter 3900: 1.576197\n",
      "iter 4000: 1.576193\n",
      "iter 4100: 1.576190\n",
      "iter 4200: 1.576186\n",
      "iter 4300: 1.576183\n",
      "iter 4400: 1.576180\n",
      "iter 4500: 1.576177\n",
      "iter 4600: 1.576174\n",
      "iter 4700: 1.576171\n",
      "iter 4800: 1.576168\n",
      "iter 4900: 1.576166\n",
      "iter 5000: 1.576164\n",
      "iter 5100: 1.576162\n",
      "iter 5200: 1.576159\n",
      "iter 5300: 1.576157\n",
      "iter 5400: 1.576156\n",
      "iter 5500: 1.576154\n",
      "iter 5600: 1.576152\n",
      "iter 5700: 1.576150\n",
      "iter 5800: 1.576149\n",
      "iter 5900: 1.576147\n",
      "iter 6000: 1.576146\n",
      "iter 6100: 1.576145\n",
      "iter 6200: 1.576143\n",
      "iter 6300: 1.576142\n",
      "iter 6400: 1.576141\n",
      "iter 6500: 1.576140\n",
      "iter 6600: 1.576139\n",
      "iter 6700: 1.576138\n",
      "iter 6800: 1.576137\n",
      "iter 6900: 1.576136\n",
      "iter 7000: 1.576135\n",
      "iter 7100: 1.576135\n",
      "iter 7200: 1.576134\n",
      "iter 7300: 1.576133\n",
      "iter 7400: 1.576132\n",
      "iter 7500: 1.576132\n",
      "iter 7600: 1.576131\n",
      "iter 7700: 1.576131\n",
      "iter 7800: 1.576130\n",
      "iter 7900: 1.576130\n",
      "iter 8000: 1.576129\n",
      "iter 8100: 1.576129\n",
      "iter 8200: 1.576128\n",
      "iter 8300: 1.576128\n",
      "iter 8400: 1.576127\n",
      "iter 8500: 1.576127\n",
      "iter 8600: 1.576126\n",
      "iter 8700: 1.576126\n",
      "iter 8800: 1.576126\n",
      "iter 8900: 1.576125\n",
      "iter 9000: 1.576125\n",
      "iter 9100: 1.576125\n",
      "iter 9200: 1.576125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9300: 1.576124\n",
      "iter 9400: 1.576124\n",
      "iter 9500: 1.576124\n",
      "iter 9600: 1.576124\n",
      "iter 9700: 1.576123\n",
      "iter 9800: 1.576123\n",
      "iter 9900: 1.576123\n",
      "iter 10000: 1.576123\n",
      "Train accuracy (%): 27.071629\n",
      "Dev accuracy (%): 25.340599\n",
      "\n",
      "=== Recap ===\n",
      "Reg\t\tTrain\t\tDev\n",
      "0.000000E+00\t28.862360\t30.245232\n",
      "1.000000E-05\t28.768727\t29.972752\n",
      "3.000000E-05\t28.651685\t29.881926\n",
      "1.000000E-04\t28.604869\t29.155313\n",
      "3.000000E-04\t27.949438\t26.793824\n",
      "1.000000E-03\t27.118446\t25.249773\n",
      "3.000000E-03\t27.083333\t25.340599\n",
      "1.000000E-02\t27.071629\t25.340599\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'getSentenceFeature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4bcdc6da5432>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnTest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestLabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[0mtestFeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetSentenceFeature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordVectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmaxRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestFeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestLabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBEST_WEIGHTS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'getSentenceFeature' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from q3_sgd import load_saved_params, sgd\n",
    "from q1_softmax import softmax\n",
    "\n",
    "import imp\n",
    "#imp.reload(q3_sgd.load_saved_params)\n",
    "#from q4_softmaxreg import softmaxRegression, getSentenceFeature, accuracy\n",
    "\n",
    "def softmaxRegression(features, labels, weights, regularization = 0.0, nopredictions = False):\n",
    "    \"\"\" Softmax Regression \"\"\"\n",
    "    # 完成加正则化的softmax回归        \n",
    "\n",
    "    # 输入:                                                         \n",
    "    # - features: feature vectors, each row is a feature vector \n",
    "    # - labels: labels corresponding to the feature vectors     \n",
    "    # - weights: weights of the regressor                       \n",
    "    # - regularization: L2 regularization constant              \n",
    "\n",
    "    # 输出:                                                         \n",
    "    # - cost: cost of the regressor                             \n",
    "    # - grad: gradient of the regressor cost with respect to its weights                                               \n",
    "    # - pred: label predictions of the regressor (you might find np.argmax helpful)  \n",
    "\n",
    "    prob = softmax(features.dot(weights))\n",
    "    if len(features.shape) > 1:\n",
    "        N = features.shape[0]\n",
    "    else:\n",
    "        N = 1\n",
    "    # A vectorized implementation of    1/N * sum(cross_entropy(x_i, y_i)) + 1/2*|w|^2\n",
    "    cost = np.sum(-np.log(prob[range(N), labels])) / N \n",
    "    cost += 0.5 * regularization * np.sum(weights ** 2)\n",
    "\n",
    "    grad = np.array(prob)\n",
    "    grad[range(N), labels] -= 1.0\n",
    "    grad = features.T.dot(grad) / N\n",
    "    grad += regularization * weights\n",
    "\n",
    "    if N > 1:\n",
    "        pred = np.argmax(prob, axis=1)\n",
    "    else:\n",
    "        pred = np.argmax(prob)\n",
    "\n",
    "    if nopredictions:\n",
    "        return cost, grad\n",
    "    else:\n",
    "        return cost, grad, pred\n",
    "\n",
    "def softmax_wrapper(features, labels, weights, regularization = 0.0):\n",
    "    cost, grad, _ = softmaxRegression(features, labels, weights, \n",
    "        regularization)\n",
    "    return cost, grad\n",
    "\n",
    "# 试试不同的正则化系数，选最好的\n",
    "REGULARIZATION = [0.0, 0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01]\n",
    "\n",
    "# 载入数据集\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# 载入预训练好的词向量 \n",
    "_, wordVectors0, _ = load_saved_params()\n",
    "wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n",
    "dimVectors = wordVectors.shape[1]\n",
    "\n",
    "# 载入训练集\n",
    "trainset = dataset.getTrainSentences()\n",
    "nTrain = len(trainset)\n",
    "trainFeatures = np.zeros((nTrain, dimVectors))\n",
    "trainLabels = np.zeros((nTrain,), dtype=np.int32)\n",
    "for i in range(nTrain):\n",
    "    words, trainLabels[i] = trainset[i]\n",
    "    trainFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n",
    "\n",
    "# 准备好训练集的特征\n",
    "devset = dataset.getDevSentences()\n",
    "nDev = len(devset)\n",
    "devFeatures = np.zeros((nDev, dimVectors))\n",
    "devLabels = np.zeros((nDev,), dtype=np.int32)\n",
    "for i in range(nDev):\n",
    "    words, devLabels[i] = devset[i]\n",
    "    devFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n",
    "\n",
    "# 尝试不同的正则化系数\n",
    "results = []\n",
    "for regularization in REGULARIZATION:\n",
    "    np.random.seed(3141)\n",
    "    np.random.seed(59265)\n",
    "    weights = np.random.randn(dimVectors, 5)\n",
    "    print(\"Training for reg=%f\" % regularization )\n",
    "\n",
    "    # batch optimization\n",
    "    weights = sgd(lambda weights: softmax_wrapper(trainFeatures, trainLabels, \n",
    "        weights, regularization), weights, 3.0, 10000, PRINT_EVERY=100)\n",
    "\n",
    "    # 训练集上测效果\n",
    "    _, _, pred = softmaxRegression(trainFeatures, trainLabels, weights)\n",
    "    trainAccuracy = accuracy(trainLabels, pred)\n",
    "    print(\"Train accuracy (%%): %f\" % trainAccuracy)\n",
    "\n",
    "    # dev集合上看效果\n",
    "    _, _, pred = softmaxRegression(devFeatures, devLabels, weights)\n",
    "    devAccuracy = accuracy(devLabels, pred)\n",
    "    print(\"Dev accuracy (%%): %f\" % devAccuracy)\n",
    "\n",
    "    # 保存结果权重\n",
    "    results.append({\n",
    "        \"reg\" : regularization, \n",
    "        \"weights\" : weights, \n",
    "        \"train\" : trainAccuracy, \n",
    "        \"dev\" : devAccuracy})\n",
    "\n",
    "# 输出准确率\n",
    "print(\"\")\n",
    "print(\"=== Recap ===\")\n",
    "print(\"Reg\\t\\tTrain\\t\\tDev\")\n",
    "for result in results:\n",
    "    print(\"%E\\t%f\\t%f\" % (\n",
    "        result[\"reg\"], \n",
    "        result[\"train\"], \n",
    "        result[\"dev\"]))\n",
    "print(\"\")\n",
    "\n",
    "# 选最好的正则化系数\n",
    "BEST_REGULARIZATION = None\n",
    "BEST_WEIGHTS = None\n",
    "\n",
    "best_dev = 0\n",
    "for result in results:\n",
    "    if result[\"dev\"] > best_dev:\n",
    "        best_dev = result[\"dev\"]\n",
    "        BEST_REGULARIZATION = result[\"reg\"]\n",
    "        BEST_WEIGHTS = result[\"weights\"]\n",
    "\n",
    "# Test your findings on the test set\n",
    "testset = dataset.getTestSentences()\n",
    "nTest = len(testset)\n",
    "testFeatures = np.zeros((nTest, dimVectors))\n",
    "testLabels = np.zeros((nTest,), dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regularization value: 0.000000E+00\n",
      "Test accuracy (%): 28.099548\n"
     ]
    }
   ],
   "source": [
    "def getSentenceFeature(tokens, wordVectors, sentence):\n",
    "    \"\"\" \n",
    "        简单粗暴的处理方式，直接对句子的所有词向量求平均做为情感分析的输入\n",
    "    \"\"\"\n",
    "\n",
    "    # 输入:                                                         \n",
    "    # - tokens: a dictionary that maps words to their indices in the word vector list                                \n",
    "    # - wordVectors: word vectors (each row) for all tokens \n",
    "    # - sentence: a list of words in the sentence of interest \n",
    "\n",
    "    # 输出:                                                         \n",
    "    # - sentVector: feature vector for the sentence    \n",
    "\n",
    "    sentVector = np.zeros((wordVectors.shape[1],))\n",
    "\n",
    "    indices = [tokens[word] for word in sentence]\n",
    "    sentVector = np.mean(wordVectors[indices, :], axis=0)\n",
    "\n",
    "    return sentVector\n",
    "\n",
    "\n",
    "for i in range(nTest):\n",
    "    words, testLabels[i] = testset[i]\n",
    "    testFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "\n",
    "_, _, pred = softmaxRegression(testFeatures, testLabels, BEST_WEIGHTS)\n",
    "print(\"Best regularization value: %E\" % BEST_REGULARIZATION)\n",
    "print(\"Test accuracy (%%): %f\" % accuracy(testLabels, pred))\n",
    "\n",
    "# 画出正则化和准确率的关系\n",
    "plt.plot(REGULARIZATION, [x[\"train\"] for x in results])\n",
    "plt.plot(REGULARIZATION, [x[\"dev\"] for x in results])\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"regularization\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.savefig(\"q4_reg_v_acc_backup.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-6 混淆矩阵\n",
    "\n",
    "上述运行程序过程已经保存confusion matrix的结果图：\n",
    "\n",
    "![q4_dev_conf.png](q4_dev_conf.png)\n",
    "\n",
    "这个矩阵的主对角线上的元素越多，说明预测越正确。其他元素都是失误。可见模型很难分辨“中性”情感，并倾向于将其分入负面。但模型没有犯下大是大非的错误（将--分入++，或反之）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-7 错误分析\n",
    "\n",
    "查看q4_dev_pred.txt中的输出，想想看什么特征可能提高效果？\n",
    "“超级消极”，“比较消极”，“中立”，“积极”，“非常积极”\n",
    "对其分别进行从0到4的编码\n",
    "\n",
    "比如：\n",
    "例子：\n",
    "1\t3     nothing is sacred in this gut-buster \n",
    "标注是1，预测是3。\n",
    "\n",
    "说反话，词袋模型的软肋\n",
    "\n",
    "再比如：\n",
    "\n",
    "3\t1\tand if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins\n",
    "标注是3，预测是1。\n",
    "\n",
    "不理解习语“moved to tears”，不理解整句话\n",
    "\n",
    "3\t0\t... routine , harmless diversion and little else\n",
    "标注是3，预测是0.\n",
    "\n",
    "语料标注错误还是？？\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

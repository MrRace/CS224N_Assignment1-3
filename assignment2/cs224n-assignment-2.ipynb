{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先在TensorFlow上实现多项逻辑斯谛回归练练手，然后增加难度实现基于神经网络的转移依存句法分析，试验Xavier初始化、Dropout和Adam优化器。最后推导RNN和语言模型的困惑度、梯度、反向传播和复杂度。\n",
    "\n",
    "\n",
    "正式开始利用TensorFlow了，这里采用最新的TensorFlow-r1.2和Python2.7。如果在安装或升级TF的过程中遇到权限问题，比如：\n",
    "`IOError: [Errno 13] Permission denied: '/usr/local/bin/markdown_py'`\n",
    "不如试试：\n",
    "\n",
    "`pip install tensorflow --user`\n",
    "\n",
    "或[《从源码编译安装TensorFlow》](http://www.hankcs.com/ml/compile-and-install-tensorflow-from-source.html)，自行编译安装得到的库效率更高。\n",
    "\n",
    "一些TensorFlow函数假定输入为行向量，所以乘上权值矩阵的时候必须右乘$xW+b$\n",
    "）而不是左乘$(Wx+b)$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils.general_utils import test_all_close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1 Tensorflow Softmax**\n",
    "\n",
    "以交叉熵损失函数:\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\tJ(\\boldsymbol{W}) &= CE(\\boldsymbol{y}, softmax(\\boldsymbol{xW})) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "实现线性分类器。要求使用TensorFlow的自动微分功能将模型拟合到数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1-1 softmax**\n",
    "\n",
    "手写实现softmax，不要用直接tf.nn.softmax。如果输入是矩阵，则视作不相干的行向量集合。\n",
    "softmax的公式如下：\n",
    "\n",
    "$$\n",
    "softmax{(x)_i}=\\frac{e^{x_i}}{\\sum_je^{x_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax function in tensorflow.\n",
    "    You might find the tensorflow functions tf.exp, tf.reduce_max,\n",
    "    tf.reduce_sum, tf.expand_dims useful. (Many solutions are possible, so you may\n",
    "    not need to use all of these functions). Recall also that many common\n",
    "    tensorflow operations are sugared (e.g. x * y does a tensor multiplication\n",
    "    if x and y are both tensors). Make sure to implement the numerical stability\n",
    "    fixes as in the previous homework!\n",
    "    Args:\n",
    "        x:   tf.Tensor with shape (n_samples, n_features). Note feature vectors are\n",
    "                  represented by row-vectors. (For simplicity, no need to handle 1-d\n",
    "                  input as in the previous homework)\n",
    "    Returns:\n",
    "        out: tf.Tensor with shape (n_sample, n_features). You need to construct this\n",
    "                  tensor in this problem.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    x_max = tf.reduce_max(x,1,keep_dims=True)          # find row-wise maximums\n",
    "    x_sub = tf.subtract(x,x_max)                            # subtract maximums\n",
    "    x_exp = tf.exp(x_sub)                              # exponentiation\n",
    "    sum_exp = tf.reduce_sum(x_exp,1,keep_dims=True)    # row-wise sums\n",
    "    out = tf.div(x_exp,sum_exp)                        # divide\n",
    "    ### END YOUR CODE\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax test 1 passed!\n",
      "Softmax test 2 passed!\n",
      "Basic (non-exhaustive) softmax tests pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#测试结果：\n",
    "\n",
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests of softmax to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "\n",
    "    test1 = softmax(tf.constant(np.array([[1001, 1002], [3, 4]]), dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "            test1 = sess.run(test1)\n",
    "    test_all_close(\"Softmax test 1\", test1, np.array([[0.26894142,  0.73105858],\n",
    "                                                      [0.26894142,  0.73105858]]))\n",
    "\n",
    "    test2 = softmax(tf.constant(np.array([[-1001, -1002]]), dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "            test2 = sess.run(test2)\n",
    "    test_all_close(\"Softmax test 2\", test2, np.array([[0.73105858, 0.26894142]]))\n",
    "\n",
    "    print(\"Basic (non-exhaustive) softmax tests pass\\n\")\n",
    "\n",
    "test_softmax_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2 交叉熵\n",
    "\n",
    "在脚本`q1_softmax.py`中，通过TensorFlow来构造交叉熵损失函数（Cross Entropy Loss）。交叉熵损失函数的公式:\n",
    "\n",
    "$$\n",
    "CE(y,\\hat{y})=-\\sum_{i=1}^{N_c}{y_ilog(\\hat{y}_i)}\n",
    "$$\n",
    "\n",
    "在这里$y\\in\\mathbb{R}^5$是一个one-hot标签向量，$N_c$是所有类目的数量。请注意，你不能使用`Tensorflow`内建的`cross-entropy`函数或者其他相关内建函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y, yhat):\n",
    "    \"\"\"\n",
    "    Compute the cross entropy loss in tensorflow.\n",
    "    The loss should be summed over the current minibatch.\n",
    "    y is a one-hot tensor of shape (n_samples, n_classes) and yhat is a tensor\n",
    "    of shape (n_samples, n_classes). y should be of dtype tf.int32, and yhat should\n",
    "    be of dtype tf.float32.\n",
    "    The functions tf.to_float, tf.reduce_sum, and tf.log might prove useful. (Many\n",
    "    solutions are possible, so you may not need to use all of these functions).\n",
    "    Note: You are NOT allowed to use the tensorflow built-in cross-entropy\n",
    "                functions.\n",
    "    Args:\n",
    "        y:    tf.Tensor with shape (n_samples, n_classes). One-hot encoded.\n",
    "        yhat: tf.Tensorwith shape (n_sample, n_classes). Each row encodes a\n",
    "                    probability distribution and should sum to 1.\n",
    "    Returns:\n",
    "        out:  tf.Tensor with shape (1,) (Scalar output). You need to construct this\n",
    "                    tensor in the problem.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    l_yhat = tf.log(yhat)                           # log yhat\n",
    "    product = tf.multiply(tf.to_float(y), l_yhat)        # multiply element-wise\n",
    "    out = tf.negative(tf.reduce_sum(product))            # negative summation to scalar\n",
    "    ### END YOUR CODE\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy test 1 passed!\n",
      "Basic (non-exhaustive) cross-entropy tests pass\n"
     ]
    }
   ],
   "source": [
    "#测试结果：\n",
    "#注意不同版本的TensorFlow在函数调用上可能有区别\n",
    "def test_cross_entropy_loss_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests of cross_entropy_loss to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    y = np.array([[0, 1], [1, 0], [1, 0]])\n",
    "    yhat = np.array([[.5, .5], [.5, .5], [.5, .5]])\n",
    "\n",
    "    test1 = cross_entropy_loss(\n",
    "            tf.constant(y, dtype=tf.int32),\n",
    "            tf.constant(yhat, dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "        test1 = sess.run(test1)\n",
    "    expected = -3 * np.log(.5)\n",
    "    test_all_close(\"Cross-entropy test 1\", test1, expected)\n",
    "\n",
    "    print(\"Basic (non-exhaustive) cross-entropy tests pass\")\n",
    "    \n",
    "test_cross_entropy_loss_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1-3  Placeholders & Feed Dictionaries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仔细学习 `assignment2/model.py`脚本中的model类。并简要解释一下其中占位符变量 (place holder vaiables)和填充字典（feed dictionaries）函数的目的. 在`q1_classifier.py`中填充好`add_palceholders和creat_feed_dict`这两个函数。\n",
    "\n",
    "提示: 请注意**配置变量**（configuration variables）在`config`类中已经储存好了，而在代码中你会用到它们。 \n",
    "\n",
    "答案：\n",
    "\n",
    "在Tensorflow 计算图中，占位符变量是作为其输入节点而存在的。而填充字典函数说明了在计算的时候怎么给这些占位符（或者其他）变量赋值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1-4 Softmax & CE Loss**\n",
    "\n",
    "在脚本`ql_classifier.py`中的完成一个用`softmax`进行分类的基本流程。并在同一个脚本中的函数`add_loss_op`中补充好交叉熵损失函数。 请注意，使用本题之前提供的函数，而不是Tensorflow 内建函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_prediction_op(self):\n",
    "    \"\"\"Adds the core transformation for this model which transforms a batch of input\n",
    "    data into a batch of predictions. In this case, the transformation is a linear layer plus a\n",
    "    softmax transformation:\n",
    "    y = softmax(Wx + b)\n",
    "    Hint: Make sure to create tf.Variables as needed.\n",
    "    Hint: For this simple use-case, it's sufficient to initialize both weights W\n",
    "                and biases b with zeros.\n",
    "    Args:\n",
    "        input_data: A tensor of shape (batch_size, n_features).\n",
    "    Returns:\n",
    "        pred: A tensor of shape (batch_size, n_classes)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    with tf.variable_scope(\"softmax_transformation\"):\n",
    "        bias = tf.Variable(tf.random_uniform([self.config.n_classes]))\n",
    "        W = tf.Variable(tf.random_uniform([self.config.n_features, self.config.n_classes]))\n",
    "        z = tf.matmul(self.input_placeholder, W) + bias\n",
    "    pred = softmax(z)\n",
    "    ### END YOUR CODE\n",
    "    return pred\n",
    "\n",
    "#交叉熵损失函数\n",
    "def add_loss_op(self, pred):\n",
    "    \"\"\"Adds cross_entropy_loss ops to the computational graph.\n",
    "    Hint: Use the cross_entropy_loss function we defined. This should be a very\n",
    "                short function.\n",
    "    Args:\n",
    "        pred: A tensor of shape (batch_size, n_classes)\n",
    "    Returns:\n",
    "        loss: A 0-d tensor (scalar)\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    loss = cross_entropy_loss(self.labels_placeholder, pred)\n",
    "    ### END YOUR CODE\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-5 Training Optimizer\n",
    "\n",
    "在脚本`ql_classifier.py`中填充完成`add_training_op `函数。 并解释为什么Tensorflow 的自动微分功能能省去我们直接求梯度的工作。你可以在我们提供的数据上顺利运行`python ql_classifier.py`命令，并且确保能够跑通测试用例。这样你才能保证你的模型是合适的。 \n",
    "\n",
    "提示：请一定要使用在config类中定义的学习率. \n",
    "\n",
    "答案：\n",
    "\n",
    "只要正确定义好了计算图，Tensorflow就能自动应用反向传播算法来计算梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_training_op(self, loss):\n",
    "    \"\"\"Sets up the training Ops.\n",
    "    Creates an optimizer and applies the gradients to all trainable variables.\n",
    "    The Op returned by this function is what must be passed to the\n",
    "    `sess.run()` call to cause the model to train. See\n",
    "    https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "    for more information.\n",
    "    Hint: Use tf.train.GradientDescentOptimizer to get an optimizer object.\n",
    "                Calling optimizer.minimize() will return a train_op object.\n",
    "    Args:\n",
    "        loss: Loss tensor, from cross_entropy_loss.\n",
    "    Returns:\n",
    "        train_op: The Op for training.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    train_op = tf.train.GradientDescentOptimizer(self.config.lr).minimize(loss)\n",
    "    ### END YOUR CODE\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF会自动求偏导数，GradientDescentOptimizer负责更新参数。\n",
    "\n",
    "完整的代码运行见`ql_classifier.py`\n",
    "\n",
    "运行后输出：\n",
    "```\n",
    "Epoch 0: loss = 111.91 (0.022 sec)\n",
    "Epoch 1: loss = 36.48 (0.025 sec)\n",
    "Epoch 2: loss = 16.91 (0.030 sec)\n",
    "Epoch 3: loss = 10.26 (0.012 sec)\n",
    "Epoch 4: loss = 7.19 (0.012 sec)\n",
    "Epoch 5: loss = 5.47 (0.012 sec)\n",
    "Epoch 6: loss = 4.39 (0.011 sec)\n",
    "Epoch 7: loss = 3.66 (0.011 sec)\n",
    "Epoch 8: loss = 3.12 (0.011 sec)\n",
    "Epoch 9: loss = 2.72 (0.011 sec)\n",
    "Epoch 10: loss = 2.41 (0.011 sec)\n",
    "Epoch 11: loss = 2.16 (0.009 sec)\n",
    "Epoch 12: loss = 1.96 (0.006 sec)\n",
    "Epoch 13: loss = 1.79 (0.007 sec)\n",
    "Epoch 14: loss = 1.65 (0.007 sec)\n",
    "Epoch 15: loss = 1.53 (0.007 sec)\n",
    "Epoch 16: loss = 1.42 (0.007 sec)\n",
    "Epoch 17: loss = 1.33 (0.007 sec)\n",
    "Epoch 18: loss = 1.25 (0.006 sec)\n",
    "Epoch 19: loss = 1.18 (0.007 sec)\n",
    "Epoch 20: loss = 1.11 (0.006 sec)\n",
    "Epoch 21: loss = 1.05 (0.007 sec)\n",
    "Epoch 22: loss = 1.00 (0.007 sec)\n",
    "Epoch 23: loss = 0.95 (0.007 sec)\n",
    "Epoch 24: loss = 0.91 (0.007 sec)\n",
    "Epoch 25: loss = 0.87 (0.007 sec)\n",
    "Epoch 26: loss = 0.84 (0.006 sec)\n",
    "Epoch 27: loss = 0.80 (0.007 sec)\n",
    "Epoch 28: loss = 0.77 (0.007 sec)\n",
    "Epoch 29: loss = 0.74 (0.007 sec)\n",
    "Epoch 30: loss = 0.72 (0.007 sec)\n",
    "Epoch 31: loss = 0.69 (0.008 sec)\n",
    "Epoch 32: loss = 0.67 (0.007 sec)\n",
    "Epoch 33: loss = 0.65 (0.006 sec)\n",
    "Epoch 34: loss = 0.63 (0.006 sec)\n",
    "Epoch 35: loss = 0.61 (0.006 sec)\n",
    "Epoch 36: loss = 0.59 (0.006 sec)\n",
    "Epoch 37: loss = 0.57 (0.006 sec)\n",
    "Epoch 38: loss = 0.56 (0.006 sec)\n",
    "Epoch 39: loss = 0.54 (0.007 sec)\n",
    "Epoch 40: loss = 0.53 (0.006 sec)\n",
    "Epoch 41: loss = 0.52 (0.006 sec)\n",
    "Epoch 42: loss = 0.50 (0.006 sec)\n",
    "Epoch 43: loss = 0.49 (0.006 sec)\n",
    "Epoch 44: loss = 0.48 (0.007 sec)\n",
    "Epoch 45: loss = 0.47 (0.006 sec)\n",
    "Epoch 46: loss = 0.46 (0.006 sec)\n",
    "Epoch 47: loss = 0.45 (0.006 sec)\n",
    "Epoch 48: loss = 0.44 (0.006 sec)\n",
    "Epoch 49: loss = 0.43 (0.006 sec)\n",
    "Basic (non-exhaustive) classifier tests pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2 基于神经网络的转移依存句法分析**\n",
    "\n",
    "这部分与之前的cs224d不同，之前的课程是命名实体识别的例子。这门课程则是用转移依存句法分析作为应用的实例。\n",
    "\n",
    "依存句法分析一是判断给定的句子是否合乎语法，再是为合乎语法的句子给出句法结构。为了准确做出句子的依存关系，不少学者提出了一些方法，如基于图的方法，基于转换的方法等。这里就是采用基于转换的神经网络方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分的理论知识不是很熟需要补充。。。\n",
    "参考资料：\n",
    "\n",
    "http://www.cipsc.org.cn/qngw/?p=885\n",
    "\n",
    "https://blog.csdn.net/weixin_38889448/article/details/80009215\n",
    "\n",
    "http://www.hankcs.com/nlp/parsing/\n",
    "\n",
    "https://applenob.github.io/gen_dep_parser.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2-1 手工完成Transition-Based Dependency Parsing**\n",
    "\n",
    "对句子：\"I parsed this sentence correctly\" 以手工方式写出依存句法树，在每个步骤罗列出stack，buffers和transition信息，及其生成的new dependency信息。结果如下：\n",
    "\n",
    "\n",
    "![2-1.jpg](2-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2-2 复杂度**\n",
    "\n",
    "一个含有n个单词的句子会花费多少个步骤？\n",
    "\n",
    "解答：\n",
    "\n",
    "句子中每个单词都必须shift到stack中，再进行一步的reduced，所以2n。即每个单词shift一步，arc一步。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2-3 初始化 & parse_step**\n",
    "\n",
    "PartialParse初始化：\n",
    "完善`q2_parser_transitions.py`中的`__init __`和`parse_step`函数，以实现`transition mechanics`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __init__(self, sentence):\n",
    "    \"\"\"Initializes this partial parse.\n",
    "    Your code should initialize the following fields:\n",
    "        self.stack: The current stack represented as a list with the top of the stack as the\n",
    "                    last element of the list.\n",
    "        self.buffer: The current buffer represented as a list with the first item on the\n",
    "                     buffer as the first item of the list\n",
    "        self.dependencies: The list of dependencies produced so far. Represented as a list of\n",
    "                tuples where each tuple is of the form (head, dependent).\n",
    "                Order for this list doesn't matter.\n",
    "    The root token should be represented with the string \"ROOT\"\n",
    "    Args:\n",
    "        sentence: The sentence to be parsed as a list of words.\n",
    "                  Your code should not modify the sentence.\n",
    "    \"\"\"\n",
    "    # The sentence being parsed is kept for bookkeeping purposes. Do not use it in your code.\n",
    "    self.sentence = sentence\n",
    "    ### YOUR CODE HERE\n",
    "    self.stack = ['ROOT']\n",
    "    self.buffer = sentence[:]\n",
    "    self.dependencies = []\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转移动作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_step(self, transition):\n",
    "    \"\"\"Performs a single parse step by applying the given transition to this partial parse\n",
    "    Args:\n",
    "        transition: A string that equals \"S\", \"LA\", or \"RA\" representing the shift, left-arc,\n",
    "                    and right-arc transitions.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    if transition == \"S\":\n",
    "        self.stack.append(self.buffer[0])\n",
    "        self.buffer.pop(0)\n",
    "    elif transition == \"LA\":\n",
    "        self.dependencies.append((self.stack[-1], self.stack[-2]))\n",
    "        self.stack.pop(-2)\n",
    "    else:\n",
    "        self.dependencies.append((self.stack[-2], self.stack[-1]))\n",
    "        self.stack.pop(-1)\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2-4 Minibatch Parsing**\n",
    "\n",
    "得益于TF强大的矩阵加速能力，你可以将多个句子同时传入。算法流程如下：\n",
    "\n",
    "![2-4.jpg](2-4.jpg)\n",
    "\n",
    "代码实现如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minibatch_parse(sentences, model, batch_size):\n",
    "    \"\"\"Parses a list of sentences in minibatches using a model.\n",
    "\n",
    "    Args:\n",
    "        sentences: A list of sentences to be parsed (each sentence is a list of words)\n",
    "        model: The model that makes parsing decisions. It is assumed to have a function\n",
    "               model.predict(partial_parses) that takes in a list of PartialParses as input and\n",
    "               returns a list of transitions predicted for each parse. That is, after calling\n",
    "                   transitions = model.predict(partial_parses)\n",
    "               transitions[i] will be the next transition to apply to partial_parses[i].\n",
    "        batch_size: The number of PartialParses to include in each minibatch\n",
    "    Returns:\n",
    "        dependencies: A list where each element is the dependencies list for a parsed sentence.\n",
    "                      Ordering should be the same as in sentences (i.e., dependencies[i] should\n",
    "                      contain the parse for sentences[i]).\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # refer: https://github.com/zysalice/cs224/blob/master/assignment2/q2_parser_transitions.py\n",
    "    partial_parses = [PartialParse(s) for s in sentences]\n",
    "\n",
    "    unfinished_parse = partial_parses\n",
    "\n",
    "    while len(unfinished_parse) > 0:\n",
    "        minibatch = unfinished_parse[0:batch_size]\n",
    "        # perform transition and single step parser on the minibatch until it is empty\n",
    "        while len(minibatch) > 0:\n",
    "            transitions = model.predict(minibatch)\n",
    "            for index, action in enumerate(transitions):\n",
    "                minibatch[index].parse_step(action)\n",
    "            minibatch = [parse for parse in minibatch if len(parse.stack) > 1 or len(parse.buffer) > 0]\n",
    "\n",
    "        # move to the next batch\n",
    "        unfinished_parse = unfinished_parse[batch_size:]\n",
    "\n",
    "    dependencies = []\n",
    "    for n in range(len(sentences)):\n",
    "        dependencies.append(partial_parses[n].dependencies)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2-5 Xavier初始化**\n",
    "\n",
    "为了防止神经元之间过度依赖，常用的技巧之一是Xavier Initialization。给定m×n大小的A，按照[−ϵ,ϵ]区间的均匀分布生成每个元素$A_{ij}$：\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\t\\epsilon &= \\frac{\\sqrt{6}}{\\sqrt{m + n}} \n",
    "\\end{align}\n",
    "$$\n",
    "实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_weight_init():\n",
    "    \"\"\"Returns function that creates random tensor.\n",
    "    The specified function will take in a shape (tuple or 1-d array) and\n",
    "    returns a random tensor of the specified shape drawn from the\n",
    "    Xavier initialization distribution.\n",
    "    Hint: You might find tf.random_uniform useful.\n",
    "    \"\"\"\n",
    "    def _xavier_initializer(shape, **kwargs):\n",
    "        \"\"\"Defines an initializer for the Xavier distribution.\n",
    "        Specifically, the output should be sampled uniformly from [-epsilon, epsilon] where\n",
    "            epsilon = sqrt(6) / <sum of the sizes of shape's dimensions>\n",
    "        e.g., if shape = (2, 3), epsilon = sqrt(6 / (2 + 3))\n",
    "        This function will be used as a variable initializer.\n",
    "        Args:\n",
    "            shape: Tuple or 1-d array that species the dimensions of the requested tensor.\n",
    "        Returns:\n",
    "            out: tf.Tensor of specified shape sampled from the Xavier distribution.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        epsilon = np.sqrt(6 / np.sum(shape))\n",
    "        out = tf.Variable(tf.random_uniform(shape=shape, minval=-epsilon, maxval=epsilon))\n",
    "        ### END YOUR CODE\n",
    "        return out\n",
    "    # Returns defined initializer function.\n",
    "    return _xavier_initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2-6 Dropout**\n",
    "\n",
    "随机地将隐藏层节点的激活值以概率p设为0，然后乘上一个常量 $γ$ ，这个过程可以写作：\n",
    "$$\n",
    "\\begin{align} \n",
    "\t\\boldsymbol{h}_{drop} &= \\lambda \\boldsymbol{d} \\circ \\boldsymbol{h} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "其中，$d∈\\{0,1\\}^{D_h} (D_h 是隐藏层h 单元数)$ 是一个遮罩向量，每个元素以概率p取0，概率1−p取1。$γ$的选取要保证激活值的期望不变，即对所有$0≤i≤D_h$都有：\n",
    "$$\n",
    "\\begin{align} \n",
    "\t\\mathbb{E_{p}}[\\boldsymbol{h}_{drop}]_{i} &= \\boldsymbol{h}_{i} \n",
    "\\end{align}\n",
    "$$\n",
    "那么给定$p，γ$ 到底要取多少呢？\n",
    "$$\\gamma = \\frac{1}{p}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** 2-7 Adam Optimizer**\n",
    "\n",
    "标准的SGD更新规则：\n",
    "$$\n",
    "\\begin{align} \n",
    "\t\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\alpha \\nabla_{\\boldsymbol{\\theta}} J_{minibatch}(\\boldsymbol{\\theta}) \n",
    "\\end{align}\n",
    "$$\n",
    "Adam用的是更加复杂的更新规则，有两个额外的步骤：\n",
    "\n",
    "1、动量更新，记录一个历史梯度的均值（速度）$m$：\n",
    "$$\n",
    "\\begin{align} \n",
    "\t\t\\boldsymbol{m} &\\leftarrow \\beta_{1}\\boldsymbol{m} + (1 - \\beta_{1}) \\nabla_{\\boldsymbol{\\theta}} J_{minibatch}(\\boldsymbol{\\theta})\\\\ \n",
    "\t\t% \n",
    "\t\t\\boldsymbol{\\theta} &\\leftarrow \\boldsymbol{\\theta} - \\alpha\\boldsymbol{m} \n",
    "\t\\end{align}\n",
    "$$\n",
    "其中$β_1$ 是一个 0 到 1 (经常取 0.9)的超参数。由于$β_1$很接近1，所以每次更新量与前一次大致相同，动量减小了更新量的方差，避免了梯度的剧烈震荡。\n",
    "\n",
    "2、Adam中的学习率还是自适应的，通过记录历史梯度的长度的平方v来得到:\n",
    "$$\n",
    "\\begin{align} \n",
    "\t\t\t\\boldsymbol{m} &\\leftarrow \\beta_{1}\\boldsymbol{m} + (1 - \\beta_{1}) \\nabla_{\\boldsymbol{\\theta}} J_{minibatch}(\\boldsymbol{\\theta})\\\\ \n",
    "\t\t\t% \n",
    "\t\t\t\\boldsymbol{v} &\\leftarrow \\beta_{2}\\boldsymbol{v} + (1 - \\beta_{2})( \\nabla_{\\boldsymbol{\\theta}} J_{minibatch}(\\boldsymbol{\\theta}) \\circ \\nabla_{\\boldsymbol{\\theta}} J_{minibatch}(\\boldsymbol{\\theta})) \\\\ \n",
    "\t\t\t% \n",
    "\t\t\t\\boldsymbol{\\theta} &\\leftarrow \\boldsymbol{\\theta} - \\alpha\\boldsymbol{m} / \\sqrt{\\boldsymbol{v}} \n",
    "\t\\end{align}\n",
    "$$\n",
    "其中$∘$ 和 $/$ 都是 elementwise运算。\n",
    "在Adam中，那些梯度平均较小的参数的更新量会更大。也就是说在损失函数相对于它们的梯度很小的地方也能快速移动，能够快速逃离高原。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2-8 实现Neural Dependency Parser**\n",
    "\n",
    "要求在开发集上的UAS大于88，loss小于0.07。\n",
    "\n",
    "就贴个最主要的方法吧："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_prediction_op(self):\n",
    "    \"\"\"Adds the 1-hidden-layer NN:\n",
    "        h = Relu(xW + b1)\n",
    "        h_drop = Dropout(h, dropout_rate)\n",
    "        pred = h_dropU + b2\n",
    "    Note that we are not applying a softmax to pred. The softmax will instead be done in\n",
    "    the add_loss_op function, which improves efficiency because we can use\n",
    "    tf.nn.softmax_cross_entropy_with_logits\n",
    "    Use the initializer from q2_initialization.py to initialize W and U (you can initialize b1\n",
    "    and b2 with zeros)\n",
    "    Hint: Here are the dimensions of the various variables you will need to create\n",
    "                W:  (n_features*embed_size, hidden_size)\n",
    "                b1: (hidden_size,)\n",
    "                U:  (hidden_size, n_classes)\n",
    "                b2: (n_classes)\n",
    "    Hint: Note that tf.nn.dropout takes the keep probability (1 - p_drop) as an argument. \n",
    "        The keep probability should be set to the value of self.dropout_placeholder\n",
    "    Returns:\n",
    "        pred: tf.Tensor of shape (batch_size, n_classes)\n",
    "    \"\"\"\n",
    "    x = self.add_embedding()\n",
    "    ### YOUR CODE HERE\n",
    "    xavier = xavier_weight_init()\n",
    "    with tf.variable_scope(\"transformation\"):\n",
    "        b1 = tf.Variable(tf.random_uniform([self.config.hidden_size,]))\n",
    "        b2 = tf.Variable(tf.random_uniform([self.config.n_classes]))\n",
    "        W = xavier([self.config.n_features * self.config.embed_size, self.config.hidden_size])\n",
    "        U = xavier([self.config.hidden_size, self.config.n_classes])\n",
    "        z1 = tf.matmul(x,W) + b1\n",
    "        h = tf.nn.relu(z1)\n",
    "        h_drop = tf.nn.dropout(h,self.dropout_placeholder)\n",
    "        pred = tf.matmul(h_drop, U) + b2\n",
    "    ### END YOUR CODE\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经典的三层网络吧，加了个dropout。在完整数据集上跑完10个epoch花了1032s，并没有题目上说的要一个小时。\n",
    "输出：\n",
    "\n",
    "```\n",
    "Epoch 10 out of 10\n",
    "924/924 [============================>.] - ETA: 0s - train loss: 0.0664Evaluating on dev set\n",
    "- dev UAS: 88.45\n",
    "New best dev UAS! Saving model in ./data/weights/parser.weights\n",
    "\n",
    "================================================================================\n",
    "TESTING\n",
    "================================================================================\n",
    "Restoring the best model weights found on the dev set\n",
    "Final evaluation on test set\n",
    "- test UAS: 89.16\n",
    "Writing predictions\n",
    "Done!\n",
    "cost= 1032.8432188034058\n",
    "\n",
    "```\n",
    "\n",
    "可以自己再加个L2正则或者信手调调参，看能否继续提高。\n",
    "\n",
    "开发集loss升去了（损失函数多了一项）。考虑到Dropout其实已经是一种正则化，所以这里L2正则化其实没有什么用。\n",
    "如同[《基于神经网络的高性能依存句法分析器》](http://www.hankcs.com/nlp/parsing/neural-network-based-dependency-parser.html)所述，原论文使用了很多技巧，比如将词性等标签也向量化了，分值当然更高。另外，这里使用的词向量仅有50维，只训练了10个epoch，自然没有论文高。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 RNN和语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定一个词语的序列：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}^{(1)}, \\boldsymbol{x}^{(2)}, \\dots, \\boldsymbol{x}^{(t)}\n",
    "$$\n",
    "\n",
    "语言模型的任务就是通过建模\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\tP(\\boldsymbol{x}^{(t+1)} &= \\boldsymbol{v}_{j} \\vert \\boldsymbol{x}^{(t)}, \\dots, \\boldsymbol{x}^{(1)}) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "来预测下一个单词$\\boldsymbol{x}^{(t+1)}$，其中$\\boldsymbol{v}_{j}$是词表中的一个单词。\n",
    "\n",
    "对RNN来讲，有：\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\t\\boldsymbol{e}^{(t)} &= \\boldsymbol{x}^{(t)}\\boldsymbol{L} \\\\ \n",
    "\t% \n",
    "\t\\boldsymbol{h}^{(t)} &= \\text{sigmoid} \\left( \\boldsymbol{h}^{(t-1)}\\boldsymbol{H} + \\boldsymbol{e}^{(t)}\\boldsymbol{I} + \\boldsymbol{b}_{1} \\right)\\\\ \n",
    "\t% \n",
    "\t\\hat{\\boldsymbol{y}}^{(t)} &= \\text{softmax}  \\left( \\boldsymbol{h}^{(t)}\\boldsymbol{U} + \\boldsymbol{b}_{2} \\right) \\\\ \n",
    "\t% \n",
    "\t\\bar{P}\\left( \\boldsymbol{x}^{(t+1)} = \\boldsymbol{v}_{j} \\vert \\boldsymbol{x}^{(t)}, \\dots, \\boldsymbol{x}^{(1)} \\right) &= \\hat{\\boldsymbol{y}}_{j}^{(t)} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "其中$\\boldsymbol{h}^{(0)} = \\boldsymbol{h}_{0} \\in \\mathbb{R}^{D_{k}}$是隐藏层的初始状态，$\\boldsymbol{L}$是lookup-table，即词嵌入矩阵。\n",
    "$I$ 是输入词表征矩阵，$H$是隐藏转化矩阵，还有$U$是输出词表征矩阵。$b1,b2$是偏置值。\n",
    "\n",
    "参数维度如下：\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\t&\\boldsymbol{L} \\in \\mathbb{R}^{\\vert V \\vert \\times d} \n",
    "\t% \n",
    "\t&\\boldsymbol{H} \\in \\mathbb{R}^{D_{h} \\times D_{h}} \\\\ \n",
    "\t% \n",
    "\t% \n",
    "\t&\\boldsymbol{I} \\in \\mathbb{R}^{d \\times D_{h}} \n",
    "\t% \n",
    "\t&\\boldsymbol{b}_{1} \\in \\mathbb{R}^{D_{h}} \\\\ \n",
    "\t% \n",
    "\t% \n",
    "\t& \\boldsymbol{U} \\in \\mathbb{R}^{D_{h} \\times \\vert V \\vert} \n",
    "\t% \n",
    "\t& \\boldsymbol{b}_{2} \\in \\mathbb{R}^{\\vert V \\vert} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "d是嵌入维度，$|V|$是词表大小，$D_{h}$是隐藏单元数，即隐层的维数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出向量$\\hat{\\boldsymbol{y}}^{(t)} \\in \\mathbb{R}^{\\vert V \\vert}$是词表上的概率分布。模型通过最小化交叉熵损失训练：\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\tJ^{(t)}(\\theta) = CE(\\boldsymbol{y}^{(t)} , \\hat{\\boldsymbol{y}}^{(t)}) &= - \\sum_{j=1}^{\\vert V \\vert} y_{j}^{(t)} log(\\hat{y}_{j}^{(t)}) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "其中，$y^{(t)}$是与目标词（这里表示为$x_{t+1}$）对应的one-hot向量。正如注释[2]所描述的，它是一个对于单个样本点的损失率，然后我们对序列中全部样例的交叉熵损失值做求和（或求平均）计算，对数据集中的全部序列[4]均采取上述操作以衡量模型的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1 困惑度和交叉熵损失\n",
    "\n",
    "按惯例，语言模型的效果用困惑度衡量，其定义如下：\n",
    "\n",
    "$$\n",
    "{PP}^{(t)}\\left(y^{(t)},\\hat{y}^{(t)}\\right)=\\frac{1}{\\bar{P}(x_{t+1}^\\mathrm{pred}=x_{t+1}|x_t,\\ldots,x_1)}=\\frac{1}{\\sideset{}{_{j=1}^{|V|}} \\sum{y_j^{(t)}}\\cdot\\hat{y}^{(t)}}\\tag{16}\n",
    "$$\n",
    "\n",
    "即通过模型分布$\\bar{P}$得到词预测正确的逆概率。给出你是如何从交叉熵的损失率中得到困惑度的（提示：不要忘了向量$y^{(t)}$是one-hot形式的！），因此如果要进行最小化操作（从算术上）意味着（从几何上讲）交叉熵损失率将被最小化，就得到了在训练集上的困惑度。在这一部分要处理的是一个非常短小的问题-所以并不需要过大的困惑度！ \n",
    "\n",
    "对于一个词库$|V|$中的词汇，如果你的模型做出的预测是完全随机的，你将如何预测你的困惑度呢？计算词库规模为$|V|=2000和|V|=10000$时对应的交叉熵损失率，并且将其作为基准牢记于心。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解答：\n",
    "\n",
    "使用的样例$y^{(t)}$为one-hot模型，且假定$y_i^{(t)}$是$y^{(t)}$中唯一的非零元素。于是，记：\n",
    "\n",
    "$$\n",
    "CE(y^{(t)},\\hat{y}^{(t)})=-\\log\\hat{y}_i^{(t)}=\\log\\frac{1}{\\hat{y}^{(t)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "PP^{(t)}(y^{(t)},\\hat{y}^{(t)})=\\frac{1}{\\hat{y}_i^{(t)}}\n",
    "$$\n",
    "\n",
    "因此，上式可以被联立为如下形状：\n",
    "\n",
    "$$\n",
    "CE(y^{(t)},\\hat{y}^{(t)})=\\log PP^{(t)}(y^{(t)},\\hat{y}^{(t)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这条公式展示了交叉熵在数学意义上的最小化目标，同时表达了困惑度在几何意义上的最小值。当模型的预测值完全随机化时，会有\n",
    "$E[\\hat{y}_i^{(t)}]=\\frac{1}{|V|}$。所给的向量$y^{(t)}$是one-hot模型，所对应困惑度的期望值是$|V|$。由于困惑度的对数即为交叉熵，那么交叉熵的期望值在面对上述两个词库规模时分别应为$log2000≈7.6l和log10000≈9.21。$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2 推导梯度\n",
    "\n",
    "在时刻t关于单点全部模型参数的梯度计算如下：\n",
    "$$\n",
    "\\begin{align} \n",
    "\t\\frac{ \\partial J^{(t)} }{ \\partial \\boldsymbol{b}_{2} } \n",
    "\\quad \n",
    "\t\\frac{ \\partial J^{(t)} }{ \\partial \\boldsymbol{L}_{x^{(t)}} } \n",
    "\\quad \n",
    "\t\\frac{ \\partial J^{(t)} }{ \\partial \\boldsymbol{I} } \\bigg\\rvert_{(t)} \n",
    "\\quad \n",
    "\t\\frac{ \\partial J^{(t)} }{ \\partial \\boldsymbol{H} } \\bigg\\rvert_{(t)} \n",
    "\\quad \n",
    "\\frac{\\partial J^{(t)}}{\\partial \\boldsymbol{h}^{(t-1)}} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "其中，$\\boldsymbol{L}_{x^{(t)}}$是词嵌入矩阵$\\boldsymbol{L}$中对应到当前处理词汇$x^{(t)}$的列，符号$|(t)$表示时刻t该参数的显式梯度。（同样地，$\\boldsymbol{h}^{(t-1)}$的取值是固定的，而且你现在也不需要在早先的迭代时刻中实现反向传播算法——这是第3小节的任务）。\n",
    "\n",
    "此外，还要计算代表前层隐层权值的导数：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{J^{(t)}}}{\\partial{\\boldsymbol{h}^{(t-1)}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3-2.jpg](3-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**推导过程：**\n",
    "\n",
    "为方便，记：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{f}_{1}^{(t)} = \\boldsymbol{h}^{(t-1)}\\boldsymbol{H} + e^{(t)}\\boldsymbol{I} + \\boldsymbol{b}_{1},\n",
    "\\boldsymbol{f}_{2}^{(t)} = \\boldsymbol{h}^{(t)}\\boldsymbol{U} + \\boldsymbol{b}_{2}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用函数$\\frac{d}{dz}\\mathrm{sigmoid}(z)=\\mathrm{sigmoid}(z)(1-\\mathrm{sigmoid}(z))$\n",
    "\n",
    "计算误差：\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\t\\boldsymbol{\\delta}_{2}^{(t)} &= \\frac{\\partial J^{(t)}}{\\partial \\boldsymbol{f}_{2}^{(t)}} % \n",
    "\t\t\t\t\t  \t   = \\hat{\\boldsymbol{y}}^{(t)} - \\boldsymbol{y}^{(t)} \\nonumber \\\\ \n",
    "\t% \n",
    "\t% \n",
    "\t\\boldsymbol{\\delta}_{1}^{(t)} \n",
    "\t&= \\frac{\\partial J^{(t)}}{\\partial \\boldsymbol{f}_{1}^{(t)}} \\\\ \n",
    "\t&=\\frac{\\partial \\boldsymbol{J}^{(t)}}{\\partial \\boldsymbol{f}_2^{(t)}} \\cdot \\frac{\\partial \\boldsymbol{f}_{2}^{(t)}}{\\partial \\boldsymbol{h}^{(t)}} \\cdot \\frac{\\partial \\boldsymbol{h}^{(t)}}{\\partial \\boldsymbol{f}_{1}^{(t)}} \\\\ \n",
    "\t\t\t\t\t  \t   &= \\boldsymbol{\\delta}_{2}^{(t)} \\cdot \\frac{\\partial \\boldsymbol{f}_{2}^{(t)}}{\\partial \\boldsymbol{h}^{(t)}} \\cdot\t\t\t\\frac{\\partial \\boldsymbol{h}^{(t)}}{\\partial \\boldsymbol{f}_{1}^{(t)}} \\nonumber \\\\ \n",
    "\t\t\t\t\t  \t  &= \\boldsymbol{\\delta}_{2}^{(t)} \\cdot \\boldsymbol{U}^{T} \\circ \\boldsymbol{h}^{(t)} \\circ (1 - \\boldsymbol{h}^{(t)}) \\nonumber \n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于上述的结果，可以得出：\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\t\\frac{\\partial J^{(t)}}{\\partial \\boldsymbol{b}_{2}} &= % \n",
    "\t\t\\frac{\\partial J^{(t)}}{\\partial \\boldsymbol{f}_{2}^{(t)}} \\frac{\\partial \\boldsymbol{f}_{2}^{(t)}}{\\partial \\boldsymbol{b}_{2}} \\nonumber \\\\ \n",
    "\t% \n",
    "\t% \n",
    "\t\t\t\t\t\t\t\t\t\t   &= \\boldsymbol{\\delta}_{2}^{(t)} = \\hat{\\boldsymbol{y}}^{(t)} - \\boldsymbol{y}^{(t)} \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\t\\frac{\\partial J^{(t)}}{\\partial \\boldsymbol{L}_{x^{(t)}}} &= % \n",
    "\t\t\\frac{\\partial J^{(t)}}{\\partial \\boldsymbol{f}_{1}^{(t)}} \\frac{\\partial \\boldsymbol{f}_{1}^{(t)}}{\\partial \\boldsymbol{e}^{(t)}} % \n",
    "\t\t\t\\frac{\\partial \\boldsymbol{e}^{(t)}}{\\partial \\boldsymbol{L}_{x^{(t)}}} \\nonumber \\\\ \n",
    "\t% \n",
    "\t% \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t &= \\boldsymbol{\\delta}_{1}^{(t)} \\boldsymbol{I}^{T} \\nonumber \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里利用到了one-hot向量时$\\boldsymbol{e}^{(t)}= \\boldsymbol{L}_{x^{(t)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align} \n",
    "\t\\frac{\\partial J^{(t)}}{\\partial \\boldsymbol{I}} &= % \n",
    "\t\t\\frac{\\partial J^{(t)}}{\\partial \\boldsymbol{f}_{1}^{(t)}} \\frac{\\partial \\boldsymbol{f}_{1}^{(t)}}{\\partial \\boldsymbol{I}} \\nonumber \\\\ \n",
    "\t% \n",
    "\t% \n",
    "\t\t\t\t\t\t\t\t\t   &=  (\\boldsymbol{e}^{(t)})^{T} \\boldsymbol{\\delta}_{1}^{(t)}  \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\t\\frac{\\partial J^{(t)}}{\\partial \\boldsymbol{H}} &= % \n",
    "\t\t\\frac{\\partial J^{(t)}}{\\partial \\boldsymbol{f}_{1}^{(t)}} \\frac{\\partial \\boldsymbol{f}_{1}^{(t)}}{\\partial \\boldsymbol{H}} \\nonumber \\\\ \n",
    "\t\t% \n",
    "\t\t% \n",
    "\t\t\t\t\t\t\t\t\t   &= (\\boldsymbol{h}^{(t-1)})^{T} \\boldsymbol{\\delta}_{1}^{(t)} \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\t\\frac{\\partial J}{\\partial \\boldsymbol{h}^{(t-1)}} &= % \n",
    "\t\t\\frac{\\partial J^{(t)}}{\\partial \\boldsymbol{f}_{1}^{(t)}} \\frac{\\partial \\boldsymbol{f}_{1}^{(t)}}{\\partial \\boldsymbol{h}^{(t-1)}} \\nonumber \\\\ \n",
    "\t\t\t\t% \n",
    "\t\t\t\t% \n",
    "\t\t\t\t\t\t\t\t\t\t\t   &=  \\boldsymbol{\\delta}_{1}^{(t)} \\boldsymbol{H}^{T} \\nonumber \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3 反向传播\n",
    "\n",
    "绘制下面三次迭代所“展开”的网络，并且计算迭代时后向传播的梯度：\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\t\\frac{ \\partial J^{(t)} }{ \\partial \\boldsymbol{L}_{x^{(t-1)}} } \n",
    "\t\\quad \n",
    "\t\\frac{ \\partial J^{(t)} }{ \\partial \\boldsymbol{I} } \\bigg\\rvert_{(t-1)} \n",
    "\t\\quad \n",
    "\t\\frac{ \\partial J^{(t)} }{ \\partial \\boldsymbol{H} } \\bigg\\rvert_{(t-1)} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用\n",
    "$$\n",
    "\\boldsymbol{\\delta}^{(t-1)} = \\frac{\\partial J^{(t)}}{\\partial \\boldsymbol{h}^{(t-1)}}\n",
    "$$\n",
    "将这些梯度表达成残差的形式,来简化结果。\n",
    "\n",
    "真实梯度需要运用反向传播到$t=0$，而实践中经常只运行固定的$\\tau \\approx 5-10$步。\n",
    "\n",
    "**网络unroll的结果如下：**\n",
    "\n",
    "![3-3.jpg](3-3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**推导过程：**\n",
    "\n",
    "引入如下记号：\n",
    "$$\n",
    "\\sigma'(\\boldsymbol{f}_{1}^{(t-1)}) = \\frac{\\partial \\boldsymbol{h}^{(t-1)}}{\\partial \\boldsymbol{f}_{1}^{(t-1)}} = diag\\left(\\boldsymbol{h}^{(t-1)} \\circ (1 - \\boldsymbol{h}^{(t-1)}) \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于$\\frac{\\partial J ^{(t)}}{\\partial \\boldsymbol{L}_{x^{(t-1)}}}$有：\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\t\\frac{\\partial J ^{(t)}}{\\partial \\boldsymbol{L}_{x^{(t-1)}}} &= % \n",
    "\t\t\\frac{\\partial J ^{(t)}}{\\partial \\boldsymbol{h}^{(t-1)}} % \n",
    "\t\t\\frac{\\partial \\boldsymbol{h}^{(t-1)}}{\\partial \\boldsymbol{f}_{1}^{(t-1)}} % \n",
    "\t\t\\frac{\\partial \\boldsymbol{f}_{1}^{(t-1)}}{\\partial \\boldsymbol{L}_{x^{(t-1)}}} \\nonumber \\\\ \n",
    "\t% \n",
    "\t\t&= \\boldsymbol{\\delta}^{(t-1)} \\sigma'(\\boldsymbol{f}_{1}^{(t-1)}) \\boldsymbol{I}^{T} \\nonumber \n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于$\\boldsymbol{I}$有：\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\t\\frac{\\partial J ^{(t)}}{\\partial \\boldsymbol{I}} &= % \n",
    "\t\t\\frac{\\partial J ^{(t)}}{\\partial \\boldsymbol{h}^{(t-1)}} % \n",
    "\t\t\\frac{\\partial \\boldsymbol{h}^{(t-1)}}{\\partial \\boldsymbol{f_1}^{(t-1)}} % \n",
    "\t\t\\frac{\\partial \\boldsymbol{f_1}^{(t-1)}}{\\partial \\boldsymbol{I}} \\nonumber \\\\ \n",
    "\t% \n",
    "\t\t&=  (\\boldsymbol{e}^{(t-1)})^{T} \\boldsymbol{\\delta}_{1}^{(t-1)} \\sigma'(\\boldsymbol{f_1}^{(t-1)}) \\nonumber \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于$\\boldsymbol{H}$有：\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\t\\frac{\\partial J ^{(t)}}{\\partial \\boldsymbol{H}} &= % \n",
    "\t\t\\frac{\\partial J ^{(t)}}{\\partial \\boldsymbol{h}^{(t-1)}} % \n",
    "\t\t\\frac{\\partial \\boldsymbol{h}^{(t-1)}}{\\partial \\boldsymbol{f_1}^{(t-1)}} % \n",
    "\t\t\\frac{\\partial \\boldsymbol{f_1}^{(t-1)}}{\\partial \\boldsymbol{H}} \\nonumber \\\\ \n",
    "\t% \n",
    "\t\t&= (\\boldsymbol{h}^{(t-2)})^{T} \\boldsymbol{\\delta}^{(t-1)} \\sigma'(\\boldsymbol{f_1}^{(t-1)}) \\nonumber \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-4 计算复杂度\n",
    "\n",
    "对于给定的$h^{(t-1)}$,执行一轮前向传播计算$J^{(t)}(\\theta)$需要多少操作？执行一轮后向传播又会如何呢？执行$\\tau$轮呢？对于维度参数组$d,D_h$和$|V|$，分别来自输入词向量到隐藏层的权值矩阵(即稠密向量，词向量的维度)、隐藏层到隐藏层的权值矩阵、隐藏层到输出的权值矩阵(即词典的长度)。\n",
    "使用符号“大写-O”来表示你的答案。\n",
    "\n",
    "解答：\n",
    "\n",
    "前向传播：\n",
    "$$\n",
    "\\begin{align} \n",
    "\t O\\left( (d \\times D_{h}) + D_{h}^{2} + (D_{h} \\times |V|) \\right) \\nonumber \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播：\n",
    "\n",
    "如果对所有单词计算梯度：\n",
    "\n",
    "$$\n",
    "\\begin{align} O\\left( \\tau \\left((d \\times D_{h}) + D_{h}^{2} +  D_{h} \\times |V| \\right)\\right) \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "这是因为反向传播$\\tau$次的路径上，残差来自$\\tau$个输出单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对单个单词计算梯度：\n",
    "\n",
    "$$\n",
    "\\begin{align} O\\left( \\tau \\left((d \\times D_{h}) + D_{h}^{2}  \\right) +  D_{h} \\times |V| \\right) \\nonumber \\end{align}\n",
    "$$\n",
    "此时反向传播$\\tau$次的路径上，来自$\\tau$个输出单词的残差中对该单词之外的导数为0。\n",
    "\n",
    "由于词表远远大于隐藏层节点数$|V|>>D_h$，所以softmax层的$O(D_{h} \\times |V|)$是瓶颈。一个解决办法是用NCE代替昂贵的交叉熵损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分居然没有代码要求，看了下之前的课程是有的。感兴趣的童鞋可以参考`cs224d_assignment2_solution`文件中的`q3_RNNLM.py`进行学习。\n",
    "\n",
    "在代码`q3_RNNLM.py`中实现的模型。其中已经实现了数据加载器和其它的初始化功能代码执行命令行`python q3_RNNLM.py`将运行该模型。注意，在运行过程中你不能用到tensorflow库的内置函数，如`rnn_cell`模块。\n",
    "\n",
    "在`ptb－train`数据集中训练该模型，其中包含`Penn Treebank`中WSJ语料集的前20节语料。正如在第2部分中所说的，你应该最大限度地提高该模型在实际生产中数据集上的泛化性能（即最小化交叉熵损失率）。关于模型的性能，我们将通过未知但类似的句子集来进行评估。\n",
    "\n",
    "作业1和该作业中的q2部分所示的神经网络是判别模型：他们接收数据之后，对其做出预测。前面你实现的RNNLM模型是一个生成式模型，因为它建模了数据序列$x1,\\dots,xn$的分布状况。这就意味着我们不仅能用它去评估句子生成概率，而且还能通过它生成概率最优的语句！\n",
    "\n",
    "训练完成后，在代码`q3 RNNLM.py` 中实现函数`generate_text()`。该函数首先运行RNN前向算法，在起始标记`<eos>`处建立索引，然后从每轮迭代的$\\hat{y}^{(t)}$分布对新词$x_{t+1}$进行采样。然后把该词作为下一步的输入，重复该操作直至该模型遇到一个结束标志（结束标志记为`</eos>`）。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
